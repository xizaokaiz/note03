# Jupyter

* jupyter定义

> 开源的科学计算平台，类比ipython，可以运行代码，做笔记；
>
> 文件后缀：.ipynb

* jupyter和pycharm对比

> jupyter：探索性数据，一边分析，一边运行；
>
> pycharm：适合逻辑性强的操作（web）。

* 如何使用

> * jupter notebook
>
> 使用方式和ipython一样，但要比ipython强大（可以画图）。

* cell

> 一对In+Out会话被视作一个代码单元，称为cell。

* jupyter两种模式

> * 编辑模式：直接点击进去，可以进行编写代码，做笔记。
>
> * 命令模式：通过快捷键操作。

* 快捷键

> * 通用
>
>   Shift+Enter，执行本单元代码，并跳转到下一单元
>
>   Ctrl+Enter，执行本单元代码，留在本单元
>
> * 命令
>
>   Y，cell切换到Code模式
>
>   M，cell切换到Markdown模式
>
>   A，在当前cell的上面添加cell
>
>   B，在当前cell的下面添加cell
>
>   双击D：删除当前cell
>
> * 编辑模式
>
>   和常规方式一样

# Matplotlin

## Matplotlib之HelloWorld

* 绘图流程

> 1. 创建画布
> 2. 绘制图像
> 3. 显示图像

```python
import matplotlib.pyplot as plt

# 1. 创建画布
plt.figure()

# 2. 图像绘制
x = [1,2,3,4,5,6]
y = [3,6,3,5,3,10]
plt.plot(x,y)

# 3. 图像展示
plt.show()
```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220517212910442.png" alt="image-20220517212910442" style="zoom:67%;" />

* 三层结构

> 容器层：canvas，figure，axes。
>
> 辅助显示层：添加x轴，y轴描述，标题等内容。
>
> 图像层：绘制图像的声明。

## 折线图(plot)与基础绘图功能

### 辅助功能

* 图像保存

> `plt.savefig()`
>
> ```python
> # 1. 创建画布
> plt.figure(figsize=(20,8),dpi=100)
> 
> # 2. 图像绘制
> x = [1,2,3,4,5,6]
> y = [3,6,3,5,3,10]
> plt.plot(x,y)
> 
> # 2.1 图像保存,一定要放在show前面
> plt.savefig('E:/my_work/ML/jupyter/data/test.png')
> 
> # 3. 图像展示
> plt.show()
> ```

* 案例：显示温度变化状况

```python
import random
# 0. 生成数据
x = range(60)
y = [random.uniform(10,50) for i in x]

# 1. 创建画布
plt.figure(figsize=(20,8),dpi=100)

# 2. 图形绘制
plt.plot(x,y)

# 3. 图形展示
plt.show()
```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220517220658674.png" alt="image-20220517220658674" style="zoom: 50%;" />

* 添加自定义x,y刻度

  > `plt.xticks`，`plt.yticks`
  >
  > 注意：第一个参数必需是数字，如果不是数字，需要进行值的替换

```python
# 2.1 添加x,y轴刻度
y_ticks = range(40)
x_ticks_labels = ["11点{}分".format(i) for i in x] # x轴刻度格式化输出

plt.yticks(y_ticks[::5]) # 从头到尾，每5分割一次
plt.xticks(x[::5],x_ticks_labels[::5]) # 值必须是数字
```

![image-20220520103212013](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220520103212013.png)

* 添加网格显示

> linestyle：网格样式
>
> alpha：透明度

```python
# 2.2 添加网格
plt.grid(True, linestyle="--", alpha=1)
```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220520104018352.png" alt="image-20220520104018352" style="zoom:67%;" />

* 添加描述信息

```python
# 2.3 添加描述
plt.xlabel("时间",fontsize=20)
plt.ylabel("温度",fontsize=20)
plt.title("一小时温度变化图",fontsize=20) # 标题
```

### 图像层

* 多次plot

> 只需要重复plot即可

```python
# 2. 图形绘制
plt.plot(x,y_beijing)
plt.plot(x,y_shanghai)
```

* 显示图例

> 区分多个折线图表示什么数据，需要先在绘制时声明**label**值；
>
> color：颜色		
>
> linestyle：样式
>
> loc：图例的显示位置

```python
# 2. 图形绘制
plt.plot(x,y_beijing,label="北京",color="g",linestyle="-.") 
plt.plot(x,y_shanghai,label="上海")
# ...
# 2.4 显示图例
plt.legend(loc=0)
```

![image-20220520105227594](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220520105227594.png)

* 多个坐标系显示-plt.subplots（面向对象的画图方法）

> `flg, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,8),dpi=100)`
>
> nrows：几行
>
> ncols：几列
>
> 有些方法需要添加set_*。

```python
# 0. 生成数据
x = range(60)
y_beijing = [random.uniform(10,15) for i in x]
y_shanghai = [random.uniform(15,25) for i in x]

# 1. 创建画布
flg, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,8),dpi=100)

# 2. 图形绘制
axes[0].plot(x,y_beijing,label="北京",color="g",linestyle="-.")
axes[1].plot(x,y_shanghai,label="上海")

# 2.1 添加x,y轴刻度
y_ticks = range(40)
x_ticks_labels = ["11点{}分".format(i) for i in x] # x轴刻度格式化

axes[0].set_xticks(x[::5])
axes[0].set_yticks(y_ticks[::5])
axes[0].set_xticklabels(x_ticks_labels[::5])
axes[1].set_xticks(x[::5])
axes[1].set_yticks(y_ticks[::5])
axes[1].set_xticklabels(x_ticks_labels[::5])

# 2.2 添加网格
axes[0].grid(True, linestyle="--", alpha=1)
axes[1].grid(True, linestyle="--", alpha=1)

# 2.3 添加描述
axes[0].set_xlabel("时间",fontsize=20)
axes[0].set_ylabel("温度",fontsize=20)
axes[0].set_title("北京一小时温度变化图",fontsize=20)
axes[1].set_xlabel("时间",fontsize=20)
axes[1].set_ylabel("温度",fontsize=20)
axes[1].set_title("上海一小时温度变化图",fontsize=20)

# 2.4 显示图例
axes[0].legend()
axes[1].legend()

# 3. 图形展示
plt.show()
```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220520112041259.png" alt="image-20220520112041259" style="zoom: 50%;" />

### 折线图的应用场景

* 数学函数图像

```python
import numpy as np

# 0. 生成数据
x = np.linspace(-10,10,1000)
y = np.sin(x) # 使用sin数据

# 1. 创建画布
plt.figure(figsize=(20,8),dpi=100)

# 2. 绘制
plt.plot(x,y)
plt.grid()

# 3. 显示
plt.show()
```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220521210342990.png" alt="image-20220521210342990" style="zoom: 67%;" />

> 若需绘制$y=x^2$的图像：
>
> `y = x*x`

## 常见图形绘制

> 折线图、散点图、柱状图、直方图、饼图

* 折线图

> `plt.plot`：变化趋势

* 散点图

> `plt.scatter()`：分布规律

```python
# 0. 数据准备
x = np.random.rand(10)  # 取出10个随机数
y = x + x ** 2 - 10  # 用自定义关系确定y的值

# 1. 创建画布
plt.figure(figsize=(20,8),dpi=100)

# 2. 图像绘制（散点图）
plt.scatter(x,y)

# 3. 图像显示
plt.show()
```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220521213736810.png" alt="image-20220521213736810" style="zoom: 80%;" />

* 柱状图

> `plt.bar`：统计，对比

```python
# 0.准备数据
# 电影名字
movie_name = ['雷神3：诸神黄昏','正义联盟','东方快车谋杀案','寻梦环游记','全球风暴','降魔传','追捕','七十七天','密战','狂兽','其它']
# 横坐标
x = range(len(movie_name))
# 票房数据
y = [73853,57767,22354,15969,14839,8725,8716,8318,7916,6764,52222]

# 1. 创建画布
plt.figure(figsize=(20,8),dpi=100)

# 2. 绘制（柱状图）
plt.bar(x, y, width=0.5, color=['b','r','g','y','c','m','y','k','c','g','b'])

# 2.1 x轴
plt.xticks(x,movie_name,fontsize=15)

# 2.2 网格
plt.grid()

# 2.3 标题
plt.title("某月电影票房统计",fontsize=50)

# 3. 显示
plt.show()

```

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220523101213498.png" alt="image-20220523101213498" style="zoom: 80%;" />

* 直方图

> `plt.hist()`：统计，分布

* 饼图

> `plt.pie()`：占比

* 教程链接： https://matplotlib.org/index.html

# numpy

* 内存块风格

<img src="file:///D:/EdgeDown/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B2%E4%B9%89/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B2%E4%B9%89/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%BA%93%EF%BC%89/Numpy/images/numpy%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80.png" alt="numpy内存地址" style="zoom: 50%;" />

> 从图中我们可以看出ndarray在存储数据的时候，数据与数据的地址都是连续的，这样就给使得批量操作数组元素时速度更快。

* ndarray支持并行化运算（向量化运算）

> Numpy内置了并行运算功能，当系统有多个核心时，做某种计算时，numpy会自动做并行计算。

* 效率高于纯Python代码

> Numpy底层使用C语言编写，内部解除了GIL（全局解释器锁），其对数组的操作速度不受Python解释器的限制，所以，其效率远高于纯Python代码。

<hr {border: 10px solid red;}>

## N维数组（ndarray）

* ndarray的属性

|     属性名字     |          属性解释          |
| :--------------: | :------------------------: |
|  ndarray.shape   |       数组维度的元组       |
|   ndarray.ndim   |          数组维数          |
|   ndarray.size   |      数组中的元素数量      |
| ndarray.itemsize | 一个数组元素的长度（字节） |
|  ndarray.dtype   |       数组元素的类型       |

* ndarray的形状

```python
a = np.array([1,2,3]) # 一维
b = np.array([[1,2,3],[2,3,4]]) # 二维
c = np.array([[[1,2,3],[2,3,4]],[[3,4,5],[4,5,6]]]) # 三维
```

* ndarray的类型

```python
# 指定数组的数据类型
d = np.array([1,2,3],dtype=np.float32) 
e = np.array(["I","love","python"],dtype = np.string_) # 字符串
```

> 默认为正数 *int64* ，小数 *float64* 。

| 名称       | 描述                                             | 简写 |
| ---------- | ------------------------------------------------ | ---- |
| np.bool    | 用一个字节存储的布尔类型（True或False）          | 'b'  |
| np.int32   | 整数，-2^31 至 2^32 -1                           | 'i4' |
| np.int64   | 整数，-2^63 至 2^63 - 1                          | 'i8' |
| np.float32 | 单精度浮点数：32位，正负号1位，指数8位，精度23位 | 'f4' |
| np.string_ | 字符串                                           | 'S'  |

<hr>

## 基本操作

### 1. 生成数组的方法

* 生成0和1的数组

```python
>> ones = np.ones([4,8]) # 全是1
array([[1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 1., 1., 1., 1., 1., 1., 1.],
       [1., 1., 1., 1., 1., 1., 1., 1.]])
>> np.zeros_like(ones) # 全是0
array([[0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.]])
```

* 从现有数组生成

```python
a = np.array([[1,2,3],[4,5,6]])
a1 = np.array(a) # 深拷贝，原数组值变化，其不变
a2 = np.asarray(a) # 浅拷贝，原数组值变化，其变化
```

* 生成固定范围的数组

> #### `np.linspace (start, stop, num, endpoint)`：
>
> * start:序列的起始值
> * stop:序列的终止值
> * num:要生成的等间隔样例数量，默认为50
> * endpoint:序列中是否包含stop值，默认为ture

```python
# 生成等间隔的数组
np.linspace(0, 100, 11)
```

返回结果：

```python
array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.])
```

> ####  `np.arange(start,stop, step, dtype)`：
>
> * 创建等差数组 — 指定步长
> * 参数
>   * step:步长,默认值为1

```python
np.arange(10,50,2)
```

返回结果：

```python
array([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42,
       44, 46, 48])
```

> #### `np.logspace(start,stop, num)`：
>
> * 创建等比数列
> * 参数:
>   * num:要生成的等比数列数量，默认为50

```python
# 生成10^x次方
np.logspace(0,2,3)
```

返回结果：

```python
array([  1.,  10., 100.])
```

### 2. 生成随即数组

* **np.random.rand(*d0*, *d1*, *...*, *dn*)**

> * 返回**[0.0，1.0)**内的一组均匀分布的数
>
> ```python
> >> np.random.rand(2,3) # 返回两行三列的数据
> array([[0.5684951 , 0.54167959, 0.87486935],
>        [0.35275814, 0.29536714, 0.79956403]])
> ```



* **np.random.uniform(*low=0.0*, *high=1.0*, *size=None*)**

> * 功能：从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high.
> * 参数介绍:
>   * low: 采样下界，float类型，默认值为0；
>   * high: 采样上界，float类型，默认值为1；
>   * size: 输出样本数目，为int或元组(tuple)类型，例如，size=(m,n,k), 则输出m*n*k个样本，缺省时输出1个值。
> * 返回值：ndarray类型，其形状和参数size中描述一致。
>
> ```python
> >> np.random.uniform(low=1,high=10,size=(3,5))
> array([[1.08947912, 8.9389742 , 8.61324294, 1.74854639, 9.84517038],
>        [9.87631099, 4.79190605, 5.84091543, 1.04895862, 3.54363077],
>        [2.81067141, 4.40097743, 1.93491519, 2.96160877, 7.18487734]])
> ```

* **np.random.randint(*low*, *high=None*, *size=None*, *dtype='l'*)**

> * 从一个均匀分布中随机采样，生成一个整数或N维整数数组，
>
> * 取数范围：若high不为None时，取[low,high)之间随机整数，否则取值[0,low)之间随机整数。
>
> ```python
> >> np.random.randint(1,10,(3,5))
> array([[7, 5, 7, 6, 9],
>        [9, 5, 7, 7, 2],
>        [3, 1, 9, 1, 9]])
> ```

* 均匀分布案例

```python
# 0. 准备数据
x = np.random.uniform(-1,1,100000000)

# 1. 画布
plt.figure(figsize=(20,8),dpi=100)

# 2. 绘制
plt.hist(x, bins=1000)

# 3. 显示
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220525111115596.png" alt="image-20220525111115596" style="zoom: 80%;" />

* 正态分布

> 正态分布是一种概率分布。正态分布是具有两个参数μ和σ的连续型随机变量的分布，第一参数μ是服从正态分布的随机变量的均值，第二个参数σ是此随机变量的标准差，所以正态分布记作**N(μ，σ )**。

<img src="E:\my_work\ML\typora_image\正态分布.png" alt="img" style="zoom: 33%;" />

> **μ决定了其位置，其标准差σ**决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。
>
> 均值决定图像的左右位置；方差值越小，图形越瘦高，数据越集中；
>
> 方差值越大，图形越矮胖，数据越分散。

* 正态分布创建方式

> * **np.random.normal(loc=0.0, scale=1.0, size=None)**
>
>   loc：float
>
>    此概率分布的均值（对应着整个分布的中心centre）
>
>   scale：float
>
>    此概率分布的标准差（对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高）
>
>   size：int or tuple of ints
>
>    输出的shape，默认为None，只输出一个值
>
> ```python
> # 0. 准备数据
> x = np.random.normal(1.17, 1 ,10000000)
> 
> # 1. 画布
> plt.figure(figsize=(20,8),dpi=100)
> 
> # 2. 绘制
> plt.hist(x, bins=1000)
> 
> # 3. 显示
> plt.show()
> ```

<img src="E:\my_work\ML\typora_image\image-20220525112422499.png" alt="image-20220525112422499" style="zoom: 67%;" />

* 案例：随机生成4支股票1周的交易日涨幅数据

> 4支股票，**一周(5天)**的涨跌幅数据，如何获取？
>
> * 随机生成涨跌幅在某个正态分布内，比如均值0，方差1
>
> ```python
> stock_change = np.random.normal(0,1,(4,5))
> ```

> 返回结果：
>
> ```python
> array([[-0.23345551, -0.00395369, -1.76464015,  0.9964357 , -0.06365135],
>        [ 0.00360294, -0.34110693, -0.23996538,  1.84001782, -0.62941206],
>        [-0.03744231, -1.72518116,  0.92942412, -0.6712868 ,  0.0536683 ],
>        [ 0.78482699, -0.88208051,  0.49603685,  0.58542951, -1.89647009]])
> ```

### 3. 数据的索引、切片

* 数组切片

```python
>> stock_change[0:2,0:3] # 先对行进行索引，再进行列索引
array([[-0.23345551, -0.00395369, -1.76464015],
       [ 0.00360294, -0.34110693, -0.23996538]])
```

### 4. 形状修改

* ndarray.reshape(shape, order)

> * 返回一个具有相同数据域，但shape不一样的**视图**
> * 行、列不进行互换

```python
>> stock_change.reshape([5,4]) # 换位5行四列 
array([[-0.23345551, -0.00395369, -1.76464015,  0.9964357 ],
       [-0.06365135,  0.00360294, -0.34110693, -0.23996538],
       [ 1.84001782, -0.62941206, -0.03744231, -1.72518116],
       [ 0.92942412, -0.6712868 ,  0.0536683 ,  0.78482699],
       [-0.88208051,  0.49603685,  0.58542951, -1.89647009]])
>> stock_change.reshape([-1,2]) # 数组修改为(2,2)，-1：表示待计算
array([[-0.23345551, -0.00395369],
       [-1.76464015,  0.9964357 ],
       [-0.06365135,  0.00360294],
       [-0.34110693, -0.23996538],
       [ 1.84001782, -0.62941206],
       [-0.03744231, -1.72518116],
       [ 0.92942412, -0.6712868 ],
       [ 0.0536683 ,  0.78482699],
       [-0.88208051,  0.49603685],
       [ 0.58542951, -1.89647009]])
```

* ndarray.resize(new_shape)

> * 修改数组本身的形状（需要保持元素个数前后相同）
> * 行、列不进行互换

```python
stock_change.resize([5,4]) # 数组变为(5,4)
```

* ndarray.T

> * 数组的转置
> * 将数组的行、列进行互换

### 5. 类型修改

* ndarray.astype(type)

```python
stock_change.astype(np.int32) # 改为int型
```

* ndarray.tostring()

```python
>> arr = np.array([[[1, 2, 3], [4, 5, 6]], [[12, 3, 34], [5, 6, 7]]])
>> arr.tostring() # 转为byte型
b'\x01\x00\x00\x00\x02\x00\x00\x00\x03\x00\x00\x00\x04\x00\x00\x00\x05\x00\x00\x00\x06\x00\x00\x00\x0c\x00\x00\x00\x03\x00\x00\x00"\x00\x00\x00\x05\x00\x00\x00\x06\x00\x00\x00\x07\x00\x00\x00'
```

### 6.数组去重

* np.unique(arr)

```python
>> arr = np.array([[1,2,3,4,5],[3,4,5,6,7]])
>> np.unique(arr)
array([1, 2, 3, 4, 5, 6, 7])
```

## ndarray运算

### 1. 逻辑运算

```python
# 生成10名同学，5门功课的数据
>>> score = np.random.randint(40, 100, (10, 5))

# 取出最后4名同学的成绩，用于逻辑判断
>>> test_score = score[6:, 0:5]

# 逻辑判断, 如果成绩大于60就标记为True 否则为False
>>> test_score > 60
array([[ True,  True,  True, False,  True],
       [ True,  True,  True, False,  True],
       [ True,  True, False, False,  True],
       [False,  True,  True,  True,  True]])

# 将成绩大于60的元素赋值为1
>>> test_score[test_score > 60] = 1
>>> test_score
array([[ 1,  1,  1, 52,  1],
       [ 1,  1,  1, 59,  1],
       [ 1,  1, 44, 44,  1],
       [59,  1,  1,  1,  1]])
```

### 2. 通用判断函数

* np.all(条件)

> 所有满足要求，返回True。

* np.any(条件)

> 只要有一个满足要求，返回True。

### 3. np.where（三元运算符）

* np.where()

```python
>> np.where(stock_d > 0,1,0) # 数组中，元素大于0赋值1，否则赋值0
array([[1, 0, 1, 1, 1],
       [0, 1, 1, 1, 1]])
```

* 复合逻辑

```python
np.where(np.logical_and(stock_d > -0.5 , stock_d < 0.5),1,0) # 逻辑与
np.where(np.logical_or(stock_d > 0,stock_d < -1),2,0) #逻辑或
```

### 4. 统计运算

* 统计指标

> * min(a, axis) ——最大值
> * max(a, axis]) ——最小值
> * median(a, axis) ——中位数
>
> * mean(a, axis, dtype) ——平均值
> * std(a, axis, dtype) ——标准差
> * var(a, axis, dtype) ——方差

> * np.argmax（temp， axis=）
>   * 返回最大值的下标
>
> * np.argmin（temp， axis=）
>   * 返回最小值的下标

```python
stock_change.max(axis=1) # 返回每行的最大值
stock_c.argmax(axis=1) # 返回按行最大值下标
stock_c.argmin(axis=0) # 返回返列最小值下标
```

## 数据间运算

### 1. 数组与数的运算

```python
>> arr = np.array([1,2,3,4])
>> arr+1 # 每个元素直接+1
array([2, 3, 4, 5])
>> arr / 2
array([0.5, 1. , 1.5, 2. ])
```

### 2. 数组与数组的运算

* 广播机制

> 数组在进行矢量化运算时，**要求数组的形状是相等的**。
>
> 当形状不一致时，就会出现广播机制：
>
> ```python
> >> arr1 = np.array([[1, 2, 3, 2, 1, 4], [5, 6, 1, 2, 3, 1]])
> >> arr2 = np.array([[1], [3]])
> >> arr1+arr2 # arr1的第一行所有数全部+1，第二行全部+3
> array([[2, 3, 4, 3, 2, 5],
>        [8, 9, 4, 5, 6, 4]])
> ```
>
> 广播机制实现了两个或两个以上数组的运算，即使这些数组的shape不是完全相同的，只需要满足如下任意一个条件即可。
>
> * 数组的某一维度等长。
> * 其中一个数组的某一维度为1 。

### 3. 矩阵乘法

* np.matmul

```python
>> a = np.array([[80, 86],[82, 80],[85, 78],[90, 90],[86, 82],[82, 90],[78, 80],[92, 94]])
>> b = np.array([[0.7], [0.3]])
>> np.matmul(a,b)
array([[81.8],[81.4],[82.9],[90. ],[84.8],[84.4],[78.6],[92.6]])
```

* np.dot

> 支持与数字相乘。

```python
>> np.dot(a,10)
```

# Pandas



# 模型评估

* 模型选择：对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。

## 数据集读取

* Mnist数据是图像数据：(28,28,1)的灰度图

```python
import numpy as np
import os
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
import warnings
warnings.filterwarnings('ignore')
np.random.seed(42)

# 导入鸢尾花数据集
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784')
mnist

# 取数据
X, y = mnist["data"],mnist["target"]
X.shape
y.shape # 7000 * 1

# 构造训练集和测试集，分别取前6万和后1万
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

# 洗牌操作：打乱顺序，独立同分布
import numpy as np
shuffle_index = np.random.permutation(60000) # 随机生成排列
X_train, y_train = X_train.iloc[shuffle_index],y_train.iloc[shuffle_index] # 按随机索引洗牌
```

## 交叉验证

* 建立模型时，会将原始数据集切分成两部分：训练集和测试集。训练集用于实际构建模型，测试集用于评估模型。

  <img src="E:\my_work\ML\typora_image\image-20220614213657990.png" alt="image-20220614213657990" style="zoom: 67%;" />

* 交叉验证

> “交叉验证法”先将数据集D划分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集。	
>
> (k=5 , 10, 20)
>
> k折检查验证通常要随机使用不同的划分重复p次，最终评估结果是这p次k折交叉验证结果的均值。
>
> <img src="E:\my_work\ML\typora_image\image-20220614213736970.png" alt="image-20220614213736970" style="zoom:50%;" />

```python
# 建立模型：判断一个数字是否为5
y_train_5 = (y_train=='5')
y_test_5 = (y_train=='5')

y_train_5[:10]
'''
返回结果：
28004    False
30680    False
49100     True
48650    False
7057     False
35832    False
41897    False
3656     False
52633    False
24449    False
'''
# 导入随机梯度下降分类模型
from sklearn.linear_model import SGDClassifier
sgd_clf = SGDClassifier(max_iter=5,random_state=42) # 参数：迭代次数，随机种子(为下一次返回新的随机种子，减少随机性)
sgd_clf.fit(X_train,y_train_5) # 开始训练

# 预测数据
sgd_clf.predict([X.loc[49100]]) # X为dataFrame型，索引时需用loc

# 导入交叉验证函数
from sklearn.model_selection import cross_val_score
cross_val_score(sgd_clf,X_train,y_train,cv=3,scoring='accuracy') # 参数：模型，原始训练集，切分成3份，得分标准-准确率
'''
返回结果：array([0.8709, 0.8549, 0.8543])	三次验证的结果
'''
```

> 交叉验证最重要的就是他的验证方式，选择不同的评价方法，会产生不同的评价结果。

* 原始交叉验证代码

```python
# 原始交叉验证代码
from sklearn.model_selection import StratifiedKFold # 切分函数
from sklearn.base import clone # 克隆器

# 实例化对象
skflods = StratifiedKFold(n_splits=3,shuffle=True,random_state=42) # 参数：切分成三份，是否打乱样本，随机种子
# 遍历切分的数据
for train_index,test_index in skflods.split(X_train, y_train_5):
    clone_clf = clone(sgd_clf) # 克隆分类器
    # 获取切分的训练集，测试集
    X_train_folds = X_train.iloc[train_index] # 验证全部都在训练集中操作
    y_train_folds = y_train_5.iloc[train_index]
    X_test_folds = X_train.iloc[test_index]
    y_test_folds = y_train_5.iloc[test_index]
    
    # 开始训练
    clone_clf.fit(X_train_folds, y_train_folds) 
    # 预测数据
    y_pred = clone_clf.predict(X_test_folds) 
    n_correct = sum(y_pred == y_test_folds) # 判断预测与实际值是否相等，存储相等的个数
    print(n_correct/len(y_pred)) # 正确率
    '''
    返回结果：0.90865	0.96565		 0.959
    '''
```



## 混淆矩阵（Confusion Matrix）

* 混淆矩阵是机器学习中总结分类模型预测结果的情形分析表，以矩阵形式将数据集中的记录按照真实的类别与分类模型预测的类别判断两个标准进行汇总。其中矩阵的行表示真实值，矩阵的列表示预测值，

<img src="E:\my_work\ML\typora_image\image-20220615105458619.png" alt="image-20220615105458619" style="zoom: 80%;" />

> true_positives：正确选出女生，作为正类。
>
> false_positives：错误选出男生，作为正类。
>
> false_negatives：错误选出男生，作为负类。
>
> true_positives：正确选出男生，作为负类。

```python
from sklearn.model_selection import cross_val_predict
# 获取预测值
y_train_pred = cross_val_predict(sgd_clf,X_train,y_train_5,cv=3) # 参数：分类器，训练集，切分3份

y_train_pred.shape # 60000 * 1
X_train.shape # (60000, 784)

# 导入混淆矩阵函数
from sklearn.metrics import confusion_matrix 
confusion_matrix(y_train_5,y_train_pred) # 参数：实际值，预测值
'''
返回结果：
array([[53655,   924],
       [ 1297,  4124]], dtype=int64)
'''
```

> **结果分析**：
>
> negative class [[ **true negatives** , **false positives** ],
>
> positive class [ **false negatives** , **true positives** ]]
>
> * true_positives： 4124张被正确的分为5类别
> * false_positives：924张被错误的分为5类别（本来不是5，判断成了5）
> * false_negatives：1297张错误的分为非5类别（本来是5，判断成不是5）
> * true_negatives: 53655个数据被正确的分为非5类别
>
> 一个完美的分类器应该只有**true positives** 和 **true negatives**, 即主对角线元素不为0，其余元素为0

## 查准率与查全率（Precision and Recall）

> 查准率：类似于“检索出的信息中有多少比例是用户感兴趣的”（判断5且为正类）
>
> 查全率：类似于“用户感兴趣的信息中有多少被检索出来了”（判断非5为正类）

> $$
> precision = \frac{TP}{TP+FP} \\[2ex]
> recall = \frac{TP}{TP+FN}
> $$

<img src="E:\my_work\ML\typora_image\image-20220615203358645.png" alt="image-20220615203358645" style="zoom: 50%;" />

```python
# 导入查准率、查全率函数
from sklearn.metrics import precision_score,recall_score
precision_score(y_train_5,y_train_pred) # 查准率
# 0.8711352955725946
recall_score(y_train_5,y_train_pred) # 查全率
# 0.6496956281128943
```

* 综合考虑查准率、查全率的性能度量

> 将**Precision** 和 **Recall**结合到一个称为**F1 score** 的指标，基于查准率和查全率的调和平均。 因此，如果查准率和查全率度都很高，分类器将获得高**F1**分数。
> $$
> F_1 = \frac{2}{\frac{1}{precision}+\frac{1}{recall}}=2*\frac{precision*recall}{precision+recall}=\frac{TP}{TP+\frac{FN+FP}{2}}
> $$

```python
from sklearn.metrics import f1_score
f1_score(y_train_5,y_train_pred) # F1
# 0.7442941673710904
```

## 阈值对结果的影响

* 即决策界限：何时会将y预测为正类，何时为负类

<img src="E:\my_work\ML\typora_image\image-20220616120348034.png" alt="image-20220616120348034" style="zoom: 67%;" />

> 如图，若阈值越低，precision值变低，FP就越多；recall值就会变高，TP就越多。
>
> 阈值越高，precision值变高，FP就越少；recall值变低，TP就越少。

```python
# 预测样本的置信度得分
y_scores = sgd_clf.decision_function([X.loc[35000]])
y_scores
# array([-232764.96231592])

# 决策界限
t = 0 # 自定义阈值
y_pred = (y_scores >t)
y_pred
# array([False])
```

* 阈值函数

```python
# 导入阈值函数
from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
# 会将所有值纳入阈值的范畴
```



## ROC曲线

> **receiver operating characteristic (ROC)** 曲线是二元分类中的常用评估方法
>
> * 它与精确度/召回曲线非常相似，但ROC曲线不是绘制精确度与召回率，而是绘制**true positive rate(TPR，  真正例率)** 与 **false positive rate(FPR，假正例率)** 
> * 要绘制ROC曲线，首先需要使用**roc_curve（）**函数计算各种阈值的**TPR和FPR**：
>
> $TPR = \frac{TP}{TP+FN} $，$FPR= \frac{FP}{TN+FP} $

```python
# 导入ROC函数
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)

# 画图：ROC曲线
def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate', fontsize=16)
    plt.ylabel('True Positive Rate', fontsize=16)

plt.figure(figsize=(8, 6))
plot_roc_curve(fpr, tpr)
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220616214707166.png" alt="image-20220616214707166" style="zoom: 67%;" />

> **虚线表示纯随机分类器的ROC曲线**; 一个好的分类器尽可能远离该线（朝左上角）。
>
> 比较分类器的一种方法是测量曲线下面积（AUC）。完美分类器的ROC AUC**趋近1**，而纯随机分类器的ROC AUC**等于0.5**。 Scikit-Learn提供了计算ROC AUC的函数：

```python
# 导入AUC函数
from sklearn.metrics import roc_auc_score
roc_auc_score(y_train_5, y_scores)
# 0.9598058535696421
```

# 线性回归模型

## 线性模型

```python
# 用sklearn线性模型
from sklearn.linear_model import LinearRegression
# 实例化对象
lin_reg = LinearRegression() 
# 训练
lin_reg.fit(X,y)
# 输出偏置项
print(lin_reg.intercept_)
# 输出权重项
print(lin_reg.coef_)
'''
[4.61793367]
[[2.90962574]]
'''
```

## 批量梯度下降

```python
eta = 0.1 # 学习率
n_iterations = 1000 # 迭代次数
m = 100 # 样本个数
# 随机初始化
theta = np.random.randn(2,1) 
# 迭代梯度下降
for n_iterations in range(n_iterations):
    gradients = 2/m*X_b.T.dot((X_b.dot(theta)-y)) # 梯度下降公式
    theta = theta - eta*gradients   # 更新
```

* 学习率对结果的影响

  > 学习率应当尽可能小，随着迭代的进行应当越来越小。

```python
theta_path_bgd = []
# 画图：学习率不同的梯度下降
def plot_gradient_descent(theta,eta,theta_path = None):
    m = len(X_b) # 样本个数
    plt.plot(X,y,'b.') # 原始数据画图
    n_iterations = 1000 # 迭代次数
    # 批量梯度下降
    for n_iterations in range(n_iterations):
        y_predict = X_new_b.dot(theta) # 预测值，x*θ=y
        # 画图：预测值
        plt.plot(X_new,y_predict,'b-')
        gradients = 2/m*X_b.T.dot((X_b.dot(theta)-y)) # 梯度下降公式
        theta = theta - eta*gradients   # 更新
        # 存储theta
        if theta_path is not None:
            theta_path.append(theta)
    plt.xlabel('X_1')
    plt.axis([0,2,0,15])
    plt.title('eta = {}'.format(eta))
```

```python
theta = np.random.rand(2,1) # 随机初始化
plt.figure(figsize = (10,4))
# 构造1行3列画板，在第1个位置画图
plt.subplot(131)
plot_gradient_descent(theta,eta = 0.02)

plt.subplot(132)
plot_gradient_descent(theta,eta = 0.1)

plt.subplot(133)
plot_gradient_descent(theta,eta = 0.5)
```

<img src="E:\my_work\ML\typora_image\image-20220703104852627.png" alt="image-20220703104852627" style="zoom:67%;" />

> 由图可知，学习率较小时，迭代次数增多；较大时，可能会略过最佳位置。

## 随机梯度下降

```python
theta_path_sgd = []
m = len(X_b) # 样本个数
n_epochs = 50 # 迭代次数

t0 = 5
t1 = 50
# 衰减策略
def learning_schedule(t):
    return t0/(t1+t) # 分母越来越大，数值越来越小

theta = np.random.rand(2,1)

# 让所有样本迭代n_epochs次
for epochs in range(n_epochs):
    for i in range(m):
        # 画图：前10次epochs，前10个样本
        if epochs < 10  and i < 10:
            y_predict = X_new_b.dot(theta)
            plt.plot(X_new,y_predict,'r--')
        random_index = np.random.randint(m) # 从1~m中获得一个随机值
        xi = X_b[random_index:random_index+1] # 取出当前随机索引所对应的数据
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta)-yi)
        # 学习率衰减
        eta = learning_schedule(n_epochs * m+i)
        theta = theta - eta*gradients # 更新
        theta_path_sgd.append(theta) # 存储tehta值
plt.plot(X,y,'b.')
plt.axis([0,2,0,15])
plt.show()
```



<img src="E:\my_work\ML\typora_image\image-20220703111512097.png" alt="image-20220703111512097" style="zoom:67%;" />

> 由图，随着迭代次数，会越来越拟合数据，且每次运行时结果都不完全一致，这就是随机性。

## MiniBatch梯度下降

>  指定每次迭代的样本个数，一般为8,16,32,64,256...；
>
> 一般情况下，batch数量应当越大越好。

```python
theta_path_mgd = []
n_epochs = 50
n_iteritions = 50
minibatch = 16
theta = np.random.randn(2,1)
np.random.seed(0) # 随机种子，使每次生成的随机数都相同

t = 0 # 计数
for epochs in range(n_epochs):
    shuffled_indices = np.random.permutation(m) # 洗牌,打乱索引
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0,m,minibatch): # 从0开始，每次取16个样本
        t+=1
        xi = X_b_shuffled[i:i+minibatch] # 取当前指定的一组样本
        yi = y_shuffled[i:i+minibatch]
        gradients = 2/minibatch*X_b.T.dot((X_b.dot(theta)-y)) # 梯度下降公式
        # 学习率衰减
        eta = learning_schedule(t)
        theta = theta - eta*gradients # 更新
        theta_path_mgd.append(theta) # 存储tehta值
```

```python
>> theta
array([[4.61793367],
       [2.90962574]])
```

## 三种策略对比

```python
# 将theta数组转为ndarray类型
theta_path_bgd = np.array(theta_path_bgd)
theta_path_sgd = np.array(theta_path_sgd)
theta_path_mgd = np.array(theta_path_mgd)
```

```python
# 画图
plt.figure(figsize=(12,6))
plt.plot(theta_path_sgd[:,0],theta_path_sgd[:,1],'r-s',linewidth=1,label='SGD')
plt.plot(theta_path_mgd[:,0],theta_path_mgd[:,1],'g-+',linewidth=2,label='MINIGD')
plt.plot(theta_path_bgd[:,0],theta_path_bgd[:,1],'b-o',linewidth=3,label='BGD')
plt.legend(loc='upper left')
plt.axis([3.5,4.5,2.0,4.0])
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220706175557163.png" alt="image-20220706175557163" style="zoom:67%;" />

> BGD批量梯度下降：直接朝着正确方向走，但样本量较大时，非常耗时。
>
> MINIGD小批量梯度下降：由于样本有限，行走路线有起伏。
>
> * 实际当中用minibatch比较多，一般情况下选择batch数量应当越大越好。
>
> SGD随机梯度下降：随机性较大，可能会趋近于正确路线。

## 多项式回归

* 出现高次幂时，需要用曲线拟合

```python
# 数据准备
m = 100
X = 6*np.random.rand(m,1) - 3 # 样本范围为（-3,3）
y = 0.5 * X**2 + X + np.random.randn(m,1)
```

```python
# 画图
plt.plot(X,y,'b.')
plt.xlabel('X_1')
plt.ylabel('y')
plt.axis([-3,3,-5,10])
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220707193059695.png" alt="image-20220707193059695" style="zoom:67%;" />

```python
# 导入多项式函数
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree = 2,include_bias = False) # 实例化，最高次为2，无偏置项

X_poly = poly_features.fit_transform(X) # 拟合数据，构造特征并返回
X[0]
# array([0.03664337])
X_poly[0] # [x,x^2]
# array([0.03664337, 0.00134274])
```

> 在`X_poly`中，构造出了$x^2$的特征。

```python
# 线性回归模型
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_poly,y)
print(lin_reg.coef_)
print(lin_reg.intercept_)
'''
[[1.04138111 0.52292947]]
[0.03953664]
'''
```

> 得到回归方程：$y=1.04x+0.52x^2+0.039$ 

* 不同degree(幂次)值的效果

```python
# 管道，标准化
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
plt.figure(figsize=(12,6))
# 三种degree值
for style,width,degree in (('g-',1,50),('y-',1,2),('r-',1,1)):
    poly_features = PolynomialFeatures(degree = degree,include_bias = False)
    std = StandardScaler() 
    lin_reg = LinearRegression()
    # 建立管道：构造特征 --> 标准化 --> 建模
    polynomial_reg = Pipeline([('poly_features',poly_features),
              ('StandardScaler',std),
              ('lin_reg',lin_reg)
    ])
    polynomial_reg.fit(X,y)  # 训练
    y_test_2 = polynomial_reg.predict(X_test)  # 获得预测值
    plt.plot(X_test,y_test_2,style,label = 'degree='+str(degree),linewidth = width)
plt.plot(X,y,'b.') # 原始数据
plt.legend(loc='upper left')
plt.axis([-3,3,-5,10])
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220708182138976.png" alt="image-20220708182138976" style="zoom: 80%;" />

> degree较大时，会尽量的满足所有点，造成过拟合，在训练集上效果还不错，但在测试集上就学得太过了。
>
> 特征变换的越复杂，得到的结果过拟合风险越高，不建议做得特别复杂。

## 数据样本数量对结果的影响

```python
# 均方误差
from sklearn.metrics import mean_squared_error
# 训练集与测试集切分
from sklearn.model_selection import train_test_split

def polt_learning_curves(model,X,y):
    # 切分，训练集0.8，测试集0.2，指定种子使随机结果相同
    X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.2,random_state=0) 
    # 均方误差
    train_errors,val_errors = [],[]
    # 基于不同样本数量进行训练
    for m in range(1,len(X_train)):
        model.fit(X_train[:m],y_train[:m])
        y_train_predict = model.predict(X_train[:m]) # 训练集预测值
        y_val_predict = model.predict(X_val) # 验证集预测值
        # 均方误差是传入真实值，预测值
        train_errors.append(mean_squared_error(y_train[:m],y_train_predict[:m]))
        val_errors.append(mean_squared_error(y_val,y_val_predict))
    # 均方根误差
    plt.plot(np.sqrt(train_errors),'r-',linewidth = 1,label='train_error')
    plt.plot(np.sqrt(val_errors),'b-',linewidth = 1,label='val_errors')
    plt.xlabel('Trainsing set size')
    plt.ylabel('RMSE')
    plt.legend()
```

```python
lin_reg = LinearRegression()
polt_learning_curves(lin_reg,X,y)
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220711101104850.png" alt="image-20220711101104850" style="zoom: 80%;" />

> 数据样本较小时，训练集的rmse低（好），测试集高（不好），即训练集和测试集的误差大，过拟合风险大。
>
> 数据样本增加时，训练集与测试集的误差越来越小。

* 多项式回归的过拟合风险

```python
# 建立管道：构造特征 --> 线性模型
polynomial_reg = Pipeline([('poly_features',PolynomialFeatures(degree=10,include_bias=False)),
                           ('lin_reg',LinearRegression())])
polt_learning_curves(polynomial_reg,X,y)
plt.axis([0,80,0,5])
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220712215736943.png" alt="image-20220712215736943" style="zoom: 80%;" />

> degree值越大时，过拟合的风险也就越大。

## 正则化

> 用于解决过拟合问题。
>
> 对权重参数进行惩罚，让权重参数尽可能平滑一些。

* 岭回归

<img src="E:\my_work\ML\typora_image\image-20220712222338095.png" alt="image-20220712222338095" style="zoom: 80%;" />

> 岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项，以达到在拟合数据的同时，使模型权重尽可能小的目的。

```python
# 岭回归
from sklearn.linear_model import Ridge
np.random.seed(42) # 设置种子
# 数据准备
m = 20
X = 3*np.random.rand(m,1)
y = 0.5 * X + np.random.randn(m,1)/1.5 + 1

# 测试集
X_new = np.linspace(0,3,100).reshape(100,1)
```

```python
def plot_model(model_class,polynomial,alphas,**model_kargs): # **：接收多个参数存入字典model_kargs
    for alpha,style in zip(alphas,('b-','g-','r-')): # 正则化项，转为元组类型：[(a1,'b-'),(a2,'g-')...]
        # 实例化模型
        model = model_class(alpha,**model_kargs)
        if polynomial:
            model = Pipeline([('poly_features',PolynomialFeatures(degree=10,include_bias=False)),
              ('StandardScaler',StandardScaler()),
              ('lin_reg',model)])
        model.fit(X,y)
        y_new_regul = model.predict(X_new) # 预测值
        lw = 2 if alpha > 0 else 1
        plt.plot(X_new,y_new_regul,style,linewidth = lw,label= 'alpha={}'.format(alpha))
    plt.plot(X,y,'b.',linewidth = 3)
    plt.legend()

plt.figure(figsize=(10,5))
plt.subplot(121)
plot_model(Ridge,polynomial=False,alphas = (0,10,100))
plt.subplot(122)
plot_model(Ridge,polynomial=True,alphas = (0,10**-5,1))
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220712230941273.png" alt="image-20220712230941273" style="zoom: 80%;" />

> alpha = 0时，即无正则化项，曲线不稳定；
>
> alpha = 1e-05时，曲线较平稳；
>
> alpha = 1时，曲线更平稳，拟合效果最好。
>
> alpha值越大时，惩罚力度越大，得到的决策方程越平稳。

* Lasso

![image-20220714201411312](E:\my_work\ML\typora_image\image-20220714201411312.png)

> 就是加了一个绝对值项。

```python
# Lasso
from sklearn import linear_model

plt.figure(figsize=(10,5))
plt.subplot(121)
plot_model(linear_model.Lasso,polynomial=False,alphas = (0,0.1,100))
plt.subplot(122)
plot_model(linear_model.Lasso,polynomial=True,alphas = (0,10**-1,1))
plt.show()
```

<img src="E:\my_work\ML\typora_image\image-20220714201808236.png" alt="image-20220714201808236" style="zoom:67%;" />



# 逻辑回归模型

* 鸢尾花数据集

```python
# 导入鸢尾花数据集
from sklearn import datasets
iris = datasets.load_iris()
list(iris.keys()) # 查看属性
```

> 对于传统逻辑回归，需要对标签进行变换，也就是将正类置为1，其它置为0。

```python
X = iris['data'][:,3:] # 取一个特征：petal width
y = (iris['target'] == 2).astype(np.int) # 转为二分类问题，目标值为0,1
```

## 概率结果随特征值的变化

```python
# 导入逻辑回归模型
from sklearn.linear_model import LogisticRegression
log_res = LogisticRegression()
log_res.fit(X,y)
```

```python
# 测试集
X_new = np.linspace(0,3,1000).reshape(-1,1) # 改变维度为d行、m列 （-1表示行数自动计算，d= a*b /m ）
# 预测概率值
y_proba = log_res.predict_proba(X_new)
```

> **y_proba**有两列数据，其和为1

```python
# 画图
plt.figure(figsize=(12,5))

# 决策边界
decision_boundary = X_new[y_proba[:,1]>=0.5][0]
plt.plot([decision_boundary,decision_boundary],[-1,2],'r:',linewidth=2)

plt.plot(X_new,y_proba[:,1],'g-',label='Iris-Virginica') # 是当前类别
plt.plot(X_new,y_proba[:,0],'b-',label='Not Iris-Virginica') # 不是当前类别
plt.legend(loc = 'center left')

# 箭头
plt.arrow(decision_boundary,0.08,-0.3,0,head_width=0.05,head_length=0.1,fc = 'b',ec = 'b')
plt.arrow(decision_boundary,0.92,0.3,0,head_width=0.05,head_length=0.1,fc = 'g',ec = 'g')

plt.xlabel('Peta width(cm)',fontsize=16)
plt.ylabel('y_proba',fontsize=16)

# 备注
plt.text(decision_boundary+0.02,0.15,'Decision Boundary',fontsize=16,color='k',ha='center')
plt.axis([0,3,-0.02,1.02])
```

<img src="E:\my_work\ML\typora_image\image-20220730163833086.png" alt="image-20220730163833086" style="zoom:80%;" />

> 随着特征值的增加，判断是该类别的概率在增加；
>
> 特征值在曲线交叉部分的概率接近50%，是最难分辨的。
>
> 当特征值超过决策边界，倾向于是；反止，倾向于不是。

## 分类决策边界展示

* 构建坐标数据，在合理的范围中，根据数据来决定

* 整合坐标点，得到所有测试输入的数据坐标点
* 预测，得到所有点的概率值
* 绘制等高线，完成决策边界

```python
# 构建坐标
x0,x1 = np.meshgrid(np.linspace(2.9,7,500).reshape(-1,1),np.linspace(0.8,2.7,200).reshape(-1,1))
# x0与x1组合，类似笛卡尔积
X_new = np.c_[x0.ravel(),x1.ravel()]

# 预测概率值
y_proba = log_res.predict_proba(X_new)
```

```python
plt.figure(figsize=(10,4))
plt.plot(X[y==0,0],X[y==0,1],'bs')
plt.plot(X[y==1,0],X[y==1,1],'g^')

# 等高线
zz = y_proba[:,1].reshape(x0.shape) # 取一个特征作为正类
contour = plt.contour(x0,x1,zz,cmap=plt.cm.brg)
plt.clabel(contour)

plt.axis([2.9,7,0.8,2.7])
```

![image-20220731221241692](E:\my_work\ML\typora_image\image-20220731221241692.png)

> 等高线越接近绿色三角分类，概率值越大。

## 多分类Softmax

![image-20220802202429799](E:\my_work\ML\typora_image\image-20220802202429799.png)

> 对数据都取以e为底的操作，放大各数据之间的差距，然后进行归一化，得到概率值。

![image-20220802202816754](E:\my_work\ML\typora_image\image-20220802202816754.png)

> 通过log函数进行映射。

```python
X = iris['data'][:,(2,3)] # 取两个特征
y = iris['target']
# 建立模型
softmax_reg = LogisticRegression(multi_class='multinomial',solver='lbfgs') # 指定多分类
softmax_reg.fit(X,y)

# 打印出三个类别的概率值
softmax_reg.predict_proba([[5,2]])
# array([[2.43559894e-04, 2.14859516e-01, 7.84896924e-01]])

# 画图：决策边界
x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1, 1),
        np.linspace(0, 3.5, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]


y_proba = softmax_reg.predict_proba(X_new)
y_predict = softmax_reg.predict(X_new)

zz1 = y_proba[:, 1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)

plt.figure(figsize=(10, 4))
plt.plot(X[y==2, 0], X[y==2, 1], "g^", label="Iris-Virginica")
plt.plot(X[y==1, 0], X[y==1, 1], "bs", label="Iris-Versicolor")
plt.plot(X[y==0, 0], X[y==0, 1], "yo", label="Iris-Setosa")

from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
plt.show()
```

![image-20220802203802927](E:\my_work\ML\typora_image\image-20220802203802927.png)

# 聚类

* 聚类

> 无监督问题：数据没有标签。
>
> 聚类：相似的东西分到一组。

## K-MEANS（无监督）

* K-MEANS

> K-means 是我们最常用的基于欧式距离的聚类算法，其认为两个目标的距离越近，相似度越大。
>
> * 要得到簇的个数，需要指定k值。
>   * K为数据最终有几个分类
> * 质心：均值，即向量各维取平均即可。
> * 距离的度量：常用欧几里得距离和余弦相似度（先标准化）
> * 优化目标：$min\sum_{i=1}^k  \sum_{x∈C_i}dist(c_i,x)^2$
>   * 取样本点到质心的最小

> 工作流程：
>
> * 根据K的值，指定质心个数。
> * 根据质心，计算每个样本到达该点的距离，取最小进行分类。
> * 分类完成后，再计算一次质心，重复上述操作。
> * 直到样本点的分类不再发生变化。
>

![image-20220810190712786](../typora_image/image-20220810190712786.png)

> 劣势：
>
> * K值难确定
> * 复杂度与样本程线性关系，即样本越多，越复杂
> * 很那发现任意形状的簇



## DBSCAN（无监督）

* 核心对象：若某个点的密度达到算法设定的阈值，则其为核心点。（即r领域内点的数量不小于minPts）

> 即一个样本点在半径内包含的样本达到一定数量，则称该点为核心点。

* *ϵ* - 邻域的距离阈值：设定的半径r。

* 直接密度可达：若某点p在点q的r领域内，且q是核心点则p-q直接密度可达。

* 密度可达：若有一个点的序列 $q_0$、$q_1$、...$q_k$ ，对任意$q_i-q_i-1$是直接密度可达的，则称从$q_0$到$q_k$密度可达，可实际上是直接密度可达的“传播”。

  >  两个样本点不处在同一领域内，且需要经过另一个核心点才能到达。

* 密度想连：若从某核心点p出发，点q和点k都是密度可达的，则称点q和点k是密度相连的。
* 边界点：属于某一点类的非核心点，不能发展下线了。
* 噪音点：不属于任何一个类簇的点，从任何一个核心点出发都是密度不可达的。

![image-20221013222503032](../typora_image/image-20221013222503032.png)

> A：核心对象
>
> B，C：边界点
>
> N：离群点、

* 工作流程

> 参数D：输入数据集
>
> 参数ϵ：指定半径
>
> MinPts：密度阈值

* 参数选择

> 半径ϵ，可以根据k距离来设定：找突变点（样本点距离差异较大的点）。
>
> k距离：给定数据集$P={p(i);i=0,1,...,n}$，计算P(i)到集合D的子集S中所有点之间的距离，距离按照从小到大的顺序排序，d(k)就被称为k-距离。
>
> MinPts：k-距离中k的值，一般取小一些，要进行多次尝试。

## 数据准备

```python
# 数据集准备
from sklearn.datasets import make_blobs

# 建立5个中心点，即类别
blob_centers = np.array(
    [[0.2,2.3],
     [-1.5,2.3],
     [-2.8,1.8],
     [-2.8,2.8],
     [-2.8,1.3]])
blob_std =np.array([0.4,0.3,0.1,0.1,0.1]) 
X,y = make_blobs(n_samples=2000,centers = blob_centers,
                cluster_std = blob_std,random_state=7)
```

```python
def plot_clusters(X, y=None):
    plt.scatter(X[:, 0], X[:, 1], c=y, s=1) # c:颜色，s：面积
    plt.xlabel("$x_1$", fontsize=14)
    plt.ylabel("$x_2$", fontsize=14, rotation=0)
plt.figure(figsize=(8, 4))
plot_clusters(X)
plt.show()
```

![image-20221013222528061](../typora_image/image-20221013222528061.png)

## 决策边界

```python
# 导入模型
from sklearn.cluster import KMeans
# 质心个数
k = 5
kmeans = KMeans(n_clusters = k,random_state = 42)
y_pred = kmeans.fit_predict(X)
```

```python
# 预测的分类结果
kmeans.labels_
# 质心点
kmeans.cluster_centers_

X_new = np.array([[0,2],[3,2],[-3,3],[-3,2.5]])
kmeans.predict(X_new) # 预测
# 样本到达各个质心的距离
kmeans.transform(X_new)
```

## 结果展示

```python
# 画图：数据集
def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], 'b.', markersize=2)

# 画图：中心点
def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    
    plt.scatter(centroids[:, 0], centroids[:, 1],# 中心点位置
                marker='o', s=30, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=50, linewidths=1,
                color=cross_color, zorder=11, alpha=1)

# 画图：决策边界
def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1 # 横纵取值范围
    maxs = X.max(axis=0) + 0.1
    # 组合为棋盘
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    # 预测结果
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()]) 
    Z = Z.reshape(xx.shape)
    
    # 画图：等高线
    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom='off')
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft='off')
```

```python
plt.figure(figsize=(8, 4))
plot_decision_boundaries(kmeans, X)
plt.show()
```

<img src="../typora_image/image-20220820164322656.png" alt="image-20220820164322656" style="zoom: 80%;" />

## 更新质心所划分的结果展示

```python
kmeans_iter1 = KMeans(n_clusters = 5,
                    init = 'random',n_init = 1,max_iter=1,random_state=1)# 初始位置随机；最佳选择迭代次数；最大迭代次数
kmeans_iter2 = KMeans(n_clusters = 5,init = 'random',n_init = 1,max_iter=2,random_state=1)
kmeans_iter3 = KMeans(n_clusters = 5,init = 'random',n_init = 1,max_iter=3,random_state=1)

kmeans_iter1.fit(X)
kmeans_iter2.fit(X)
kmeans_iter3.fit(X)
```

```python
plt.figure(figsize = (12,8))


# 第一次迭代得到的中心点
plt.subplot(321)
plot_data(X) # 数据
plot_centroids(kmeans_iter1.cluster_centers_,circle_color='r', cross_color='k') # 中心点
plt.title('Update cluster_centers')


# 第一次迭代后的分类结果
plt.subplot(322)
plot_decision_boundaries(kmeans_iter1, X,  show_centroids=True, # 决策边界
                             show_xlabels=False, show_ylabels=False)
plt.title('Label ')


# 更新一次后的中心点
plt.subplot(323)
plot_decision_boundaries(kmeans_iter1, X,  show_centroids=True, 
                             show_xlabels=False, show_ylabels=False)
plot_centroids(kmeans_iter2.cluster_centers_)# 中心点

# 更新一次中心点后的分类结果
plt.subplot(324)
plot_decision_boundaries(kmeans_iter2, X,  show_centroids=True,
                             show_xlabels=False, show_ylabels=False)

# 第三次更新后的中心点
plt.subplot(325)
plot_decision_boundaries(kmeans_iter2, X,  show_centroids=True,
                             show_xlabels=False, show_ylabels=False)
plot_centroids(kmeans_iter3.cluster_centers_)

# 第三次迭代后的分类结果
plt.subplot(326)
plot_decision_boundaries(kmeans_iter3, X,  show_centroids=True,
                             show_xlabels=False, show_ylabels=False)

plt.show()
```

<img src="../typora_image/image-20220820225250673.png" alt="image-20220820225250673" style="zoom:80%;" />

> 左为更新质心的操作，右为更新质心后，对样本进行划分的分类结果。

## KMeans的不稳定性

```python
# 画图：比较两个结果
def plot_clusterer_comparison(c1,c2,X):
    c1.fit(X)
    c2.fit(X)
    
    plt.figure(figsize=(12,4))
    plt.subplot(121)
    plot_decision_boundaries(c1,X)
    plt.subplot(122)
    plot_decision_boundaries(c2,X)
```

```python
c1 = KMeans(n_clusters = 5, init = 'random',n_init = 1,random_state = 11)
c2 = KMeans(n_clusters = 5, init = 'random',n_init = 1,random_state = 19)
plot_clusterer_comparison(c1,c2,X)
```

<img src="../typora_image/image-20220820230011025.png" alt="image-20220820230011025" style="zoom:80%;" />

> 如图，算法具有不稳定性，随机的质心不同，所划分的结果也不同。
>
> 尽量将 **n_init** 的值设置高一点，减少不稳定性。

## 评估指标

* Inertia指标：每个样本与其质心的距离的平方和，越小越好。

```python
kmeans.inertia_
```

> **n_init**参数是通过n次迭代次数，取最好的一次作为结果，即选择的是**Inertia**指标最小的一次。

## 最佳质心数

* k值越大，所得评估指标越小。

  ```python
  # 建立十个分类器，对应k=1,2,...,10
  kmeans_per_k = [KMeans(n_clusters=k).fit(X) for k in range(1,10)]
  inertias = [model.inertia_ for model in kmeans_per_k]
  ```

  ```python
  plt.figure(figsize=(8,4))
  plt.plot(range(1,10),inertias,'bo-')
  plt.xlabel('k')
  plt.ylabel('inertia')
  plt.axis([1,8.5,0,1300])
  plt.show()
  ```

  <img src="../typora_image/image-20220821193127313.png" alt="image-20220821193127313" style="zoom:80%;" />

  > k=4时，为拐点，下降幅度变低。则k=4较为好。

## 轮廓系数

* 𝑎𝑖 : 计算样本i到同簇其他样本的平均距离ai。ai 越小，说明样本i越应该被聚类到该簇。将ai 称为样本i的簇内不相似度。

* 𝑏𝑖 : 计算样本i到其他某簇Cj 的所有样本的平均距离bij，称为样本i与簇Cj 的不相似度。

  * 定义为样本i的簇间不相似度：bi =min{bi1, bi2, ..., bik}

  $$
  s(i) = \frac{b(i)-a(i)}{max(a(i),b(i))} \,\,\,\,\,\,\,\,\,\,\,
  s(i)=\begin{cases}
  1- \frac{a(i)}{b(i)} & ,a(i)<b(i) \\
  0 & ,a(i)=b(i) \\
  \\frac{a(i)}{b(i)}-1 & ,a(i)>b(i) 
  \end{cases}
  $$

  > 结论：
  >
  > * si接近1，则说明样本i聚类合理；
  > * si接近-1，则说明样本i更应该分类到另外的簇；
  > * 若si 近似为0，则说明样本i在两个簇的边界上。

```python
from sklearn.metrics import silhouette_score
silhouette_score(X,kmeans.labels_) # 传入数据集和分类结果
# 0.655517642572828
```

```python
silhouette_scores = [silhouette_score(X,model.labels_) for model in kmeans_per_k[1:]]
silhouette_scores # 每一个k值对应的轮廓系数，k=2开始
'''
[0.5966442557582528,
 0.5723900247411775,
 0.6885316175957589,
 0.655517642572828,
 0.602433506629666,
 0.6068660656395705,
 0.561138795623175,
 0.5668088143650577]
'''
```

```python
plt.figure(figsize=(8,4))
plt.plot(range(2,10),silhouette_scores,'bo-')
plt.xlabel('k')
plt.ylabel('silhouette_scores')
plt.show()
```

<img src="../typora_image/image-20220824215208275.png" alt="image-20220824215208275" style="zoom:80%;" />

> 如图，k=4时，轮廓系数最接进1。

## Kmeas存在的问题

```python
# 构建数据集
X1,y1 = make_blobs(n_samples=1000, centers = ((4,-4),(0,0)),random_state=42)
X1 = X1.dot(np.array([[0.374,0.95],[0.732,0.598]]))
X2,y2 = make_blobs(n_samples=250,centers = 1,random_state=42)
X2 = X2 + [6,-8]
X = np.r_[X1,X2] # 按列拼接，上下拼接
y = np.r_[y1,y2]

plot_data(X)
```

<img src="../typora_image/image-20220824220520318.png" alt="image-20220824220520318" style="zoom:80%;" />



```python
# 对比实验
kmeans_good = KMeans(n_clusters=3,init = np.array([[-1.5,2.5],[0.5,0],[4,0]]), # 指定已知质心
                     n_init=1,random_state=42) 
kmeans_bad = KMeans(n_clusters=3,random_state=42)
kmeans_good.fit(X)
kmeans_bad.fit(X)
```

```python
plt.figure(figsize=(10,4))
plt.subplot(121)
plot_decision_boundaries(kmeans_good,X)
plt.title('Good - inertia = {}'.format(kmeans_good.inertia_))

plt.subplot(122)
plot_decision_boundaries(kmeans_bad,X)
plt.title('Bad - inertia = {}'.format(kmeans_bad.inertia_))
```

<img src="../typora_image/image-20220824220649386.png" alt="image-20220824220649386" style="zoom:80%;" />

> 如图，右图的inertia指标比左图的要小，但分类结果并不好。
>
> **评估指标只能作为参考，不能用它来绝对衡量模型。**

## 应用实例—图像分割

```python
# 导入图像
from matplotlib.image import imread
image = imread('E:/my_work/ML/部分代码资料/9-聚类算法实验分析/聚类算法-实验/ladybug.png')
image.shape # 533*800像素 RGB颜色
# (533, 800, 3)
```

```python
# 更改形状为二维的，(533*800,3)，(像素点,颜色分类)
X = image.reshape(-1,3) 
X.shape
# (426400, 3)
```

```python
# 训练
kmeans = KMeans(n_clusters=8,random_state=42).fit(X)
# 8个分类的质心位置
kmeans.cluster_centers_
```

```python
array([[0.9835153 , 0.9358979 , 0.02575099],
       [0.02285826, 0.11067167, 0.00578478],
       [0.21914628, 0.38675895, 0.05800834],
       [0.75767696, 0.21229179, 0.04460662],
       [0.09990539, 0.2542202 , 0.01693691],
       [0.612661  , 0.63010174, 0.38750824],
       [0.37211823, 0.52359724, 0.15730515],
       [0.8845895 , 0.72559094, 0.03441923]], dtype=float32)
```

```python
# 用每个分类的质心来表示数据
segmented_img =  kmeans.cluster_centers_[kmeans.labels_].reshape(533, 800, 3) # 数据都用所属簇的位置来表示
```

```python
# 对比实验
segmented_imgs = []
n_colors = (10,8,6,4,2) # 不同颜色分类
for n_cluster in n_colors:
    kmeans = KMeans(n_clusters = n_cluster,random_state=42).fit(X)
    segmented_img =  kmeans.cluster_centers_[kmeans.labels_]
    segmented_imgs.append(segmented_img.reshape(image.shape)) 
```

```python
# 画图
plt.figure(figsize=(10,5))
plt.subplot(231) # 原始图
plt.imshow(image)
plt.title('Original image') 

for idx,n_clusters in enumerate(n_colors):
    plt.subplot(232+idx)
    plt.imshow(segmented_imgs[idx]) # 取不同颜色个数分类的下标
    plt.title('{}colors'.format(n_clusters))
```

<img src="../typora_image/image-20220828211852882.png" alt="image-20220828211852882" style="zoom:80%;" />

> 可以看出，当颜色分类减少时，很明显的分离出了向日葵。

## 半监督

* 当前对样本进行随机的逻辑回归分类

```python
# 导入数据
from sklearn.datasets import load_digits
X_digits,y_digits = load_digits(return_X_y = True)

# 分割数据集
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_digits,y_digits,random_state=42)
```

```python
# 导入模型
from sklearn.linear_model import LogisticRegression
# 样本数量
n_labeled = 50

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train[:n_labeled],y_train[:n_labeled])
log_reg.score(X_test,y_test)
# 0.8266666666666667
```

* 用kmeans对比实验

```python
k = 50
kmeans = KMeans(n_clusters=k,random_state=42)
X_digits_dist = kmeans.fit_transform(X_train) # 每个样本与其质心的距离

# 取离每个簇最近的样本
representative_digits_idx =  np.argmin(X_digits_dist,axis=0) # 检索数组中最小值的位置，并返回其下标值,取列
representative_digits_idx.shape
# (50,)

# 取对应数据
X_representative_digits = X_train[representative_digits_idx]
```

```python
# 画图
plt.figure(figsize=(8,2))
for index,X_representative_digit in enumerate(X_representative_digits):
    plt.subplot(k // 10,10,index+1)
    plt.imshow(X_representative_digit.reshape(8,8),cmap="binary",
               interpolation='bilinear') # cmp:指定渐变色；interpolation:显示形式
    plt.axis('off')
    
plt.show()
```

<img src="../typora_image/image-20220831172611159.png" alt="image-20220831172611159" style="zoom:80%;" />

> 当前取出了对于每个簇具有代表性的样本，将这50个样本标记为簇心。

```python
# 手动指定出50个特征
y_representative_digits = np.array([
    4, 8, 0, 6, 8, 3, 7, 7, 9, 2,
    5, 5, 8, 5, 2, 1, 2, 9, 6, 1,
    1, 6, 9, 0, 8, 3, 0, 7, 4, 1,
    6, 5, 2, 4, 1, 8, 6, 3, 9, 2,
    4, 2, 9, 4, 7, 6, 2, 3, 1, 1])
```

* 现在我们有一个只有50个标记实例的数据集，它们中的每一个都是其集群的代表性图像，而不是完全随机的实例。 让我们看看性能是否更好：

```python
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_representative_digits, y_representative_digits)
log_reg.score(X_test, y_test)
# 0.9155555555555556
```

> 通过手动标记样本，效果要比随机的高，这就是半监督学习。

* 但也许我们可以更进一步：如果我们将标签传播到同一群集中的所有其他实例，该怎么办？

```python
y_train_propagated = np.empty(len(X_train),dtype=np.int32)
# 将特征传播到其他的样本
for i in range(k):
    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]
    
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train,y_train_propagated)
log_reg.score(X_test,y_test)
# 0.9288888888888889
```

* 只取簇中前20个来试试，也就是每个簇中距离最小的20个作为训练集

```python
precentile_closest = 20

# 取簇中样本
X_cluster_dist = X_digits_dist[np.arange(len(X_train)),kmeans.labels_]
for i in range(k): # 对每个簇进行遍历
    in_cluster = (kmeans.labels_ == i)  # 当前簇
    cluster_dist = X_cluster_dist[in_cluster] # 选择属于当前簇的所有样本
    cutoff_distance = np.percentile(cluster_dist,precentile_closest) # 对cluster_dist排序,找到每个簇的前20个样本，距离最小的
    above_cutoff = (X_cluster_dist > cutoff_distance) # False True 结果，判断，取比之前簇心距离要小的样本
    X_cluster_dist[in_cluster & above_cutoff] = -1 # 若为True,则对应位置存储相应距离，否则为-1
```

```python
partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train[partially_propagated]
```

```python
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train_partially_propagated,y_train_partially_propagated)
log_reg.score(X_test,y_test)
# 0.9488888888888889
```

> 至次，模型的效果又提高了。

## DBSCAN

```python
# 导入数据
from sklearn.datasets import make_moons
X,y =  make_moons(n_samples=1000,noise=0.05,random_state=42)
plt.plot(X[:,0],X[:,1],'b.')
```

<img src="../typora_image/image-20220901101526855.png" alt="image-20220901101526855" style="zoom:80%;" />

> 对该数据进行聚类算法分类。

```python
# 导入模型
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps = 0.05,min_samples=5) # 半径，核心点样本数
dbscan.fit(X)

# 分类结果
dbscan.labels_[:10] # -1表示离群点
# array([ 0,  2, -1, -1,  1,  0,  0,  0,  2,  5], dtype=int64)

# 核心对象的索引
dbscan.core_sample_indices_[:10]
# array([ 0,  4,  5,  6,  7,  8, 10, 11, 12, 13], dtype=int64)

# 分类后有哪些簇
np.unique(dbscan.labels_)
array([-1,  0,  1,  2,  3,  4,  5,  6], dtype=int64)
```

```python
# 半径变大，对比实验
dbscan2 = DBSCAN(eps = 0.2,min_samples=5) 
dbscan2.fit(X)
```

```python
# 画图函数
def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True # 标记
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]
    
    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask], marker='o', s=size, cmap="Paired")
    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])
    plt.scatter(anomalies[:, 0], anomalies[:, 1],
                c="r", marker="x", s=100)
    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=".")
    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom='off')
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft='off')
    plt.title("eps={:.2f}, min_samples={}".format(dbscan.eps, dbscan.min_samples), fontsize=14)
```

```python
# 画图
plt.figure(figsize=(9, 3.2))

plt.subplot(121)
plot_dbscan(dbscan, X, size=100)

plt.subplot(122)
plot_dbscan(dbscan2, X, size=600, show_ylabels=False)

plt.show()
```

<img src="../typora_image/image-20220901101820359.png" alt="image-20220901101820359" style="zoom:80%;" />

> 左图，半径较小时，分类的簇较多； 右图，半径较大时，两个簇，合适。
>
> 可以通过轮廓系数来选择半径大小。
>
> 优先用分类来解决问题，最后才选择用聚类。

# 决策树

* 从根节点开始一步一步走到叶子节点（决策）
* 所有的数据最终都会落到叶子节点，既可以做分类也可以做回归

<img src="../typora_image/image-20220904202114337.png" alt="image-20220904202114337" style="zoom:80%;" />

> 根节点的决策应较强。

* 树的组成

> 根节点：第一个节点
>
> 非叶子结点与分支：中间过程
>
> 叶子结点：最终的决策结果

<img src="../typora_image/image-20220904211711824.png" alt="image-20220904211711824" style="zoom:80%;" />

* 决策树的训练与测试

> 训练阶段：从给定的训练集构造出来一棵树（从根节点开始选择合适的特征，按优先级进行排列）
>
> 测试阶段：根据构造出来的树模型从上到下走一遍

* 如何切分特征（选择节点）

> 问题：根节点的选择该用哪个特征？接下来如何切分？
>
> 应该根据分类效果来排列先后顺序。
>
> 目标：通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找出来最好的那个当成根节点，以此类推。

* 熵

> **表示随机变量不确定性的度量，即物体内部的混乱程度。**

$$
H(X) = - \sum pi * logpi,i=1,2,...,n \\[2ex]
pi为概率，当概率越大时，熵值越小
$$

> 如：$A = \{1,1,1,1,1,1,1,2,2\}$，$ B = \{1,2,3,4,5,6,7,8,9\}$
>
> A集合中的类别少，熵值低；B集合中类别多，熵值高。
>
> 在分类任务中，通过节点分支后数据类别的熵值越小为好。

> 如图，**不确定性越大，得到的熵值也就越大**：
>
> 当p=0或p=1时，H(p) = 0，随机变量完全没有不确定性。
>
> 当p=0.5时，H(p)=1，此时随机变量的不确定性最大。

<img src="../typora_image/image-20220904213851955.png" alt="image-20220904213851955" style="zoom:80%;" />

* 信息增益

> 表示特征X使得类Y的不确定性减少的程度。
>
> 选择**信息增益**高的作为优先节点。

* 决策树算法

> ID3：信息增益
>
> C4.5：信息增益率（解决ID3问题，考虑自身熵）
>
> CART：使用GINI（基尼）系数来当做衡量标准
>
> GINI系数：$Gini(p) = \sum_{k=1}^{k}p_k(1-p_k) = 1-\sum_{k=1}^{k}p_k^2$

* 连续值怎么办？

> 连续值离散化：对数据进行排序，然后选取分界点（若为二分类）。
>
> $ 60,70,75,85,90,95,100,120,125,220$
>
>  如：$ 60|70,75,85,90,95,100,120,125,220$，将60作为分界点，计算熵值；
>
> $ 60,70|75,85,90,95,100,120,125,220$，将70作为分界点，计算熵值；
>
> 依次切分，比较即可得。

* 剪枝策略（避免过拟合）

> 决策树过拟合风险很大，理论上可以完全分得开数据。
>
> **将数据分支到极致，即分到结点只有一个样本，就造成了过拟合。**

> **预剪枝**：边建立决策树边进行剪枝的操作（更实用）。
>
> 限制深度（选取的特征个数）、叶子结点个数、叶子结点样本数、信息增益量等。
>
> 控制决策树的规模和复杂程度。

> 后剪枝：当建立完成决策树后来进行剪枝操作。
>
> 通过一定的衡量标准，$C_α(T)=C(T)+α·|T_{leaf}|$（损失=熵值+平衡系数*叶子结点个数）

* 回归问题

> 利用方差来比较连续值的划分。

## 树模型可视化展示

* 用莺尾花数据集进行展示

```python
# 导入数据集和模型
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data[:,2:] # 选择2,3列数据
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2) # 最大深度
tree_clf.fit(X,y)
```

```python
# 生成决策树模型的图片
from sklearn.tree import export_graphviz

export_graphviz(
    tree_clf, # 模型
    out_file = "iris_tree.dot",
    feature_names = iris.feature_names[2:], # 特征名
    class_names = iris.target_names, # y值的名字
    rounded = True,
    filled = True
)
```

> 会生成一个.dot的文件，然后用此命令，转为png文件：
>
> ` dot -Tpng iris_tree.dot -o iris_tree.png`

```python
# 图片展示
from IPython.display import Image
Image(filename='iris_tree.png',width=400,height=400)
```

<img src="../typora_image/image-20220913200327526.png" alt="image-20220913200327526" style="zoom: 67%;" />



## 决策边界展示

```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):
    x1s = np.linspace(axes[0], axes[1], 100) # 构造特征
    x2s = np.linspace(axes[2], axes[3], 100)
    x1, x2 = np.meshgrid(x1s, x2s) # 棋盘
    X_new = np.c_[x1.ravel(), x2.ravel()] # 测试集
    y_pred = clf.predict(X_new).reshape(x1.shape) # 预测值
    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])
    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)
    if not iris:
        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)
    if plot_training: # 画图：训练数据
        plt.plot(X[:, 0][y==0], X[:, 1][y==0], "yo", label="Iris-Setosa")
        plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs", label="Iris-Versicolor")
        plt.plot(X[:, 0][y==2], X[:, 1][y==2], "g^", label="Iris-Virginica")
        plt.axis(axes)
    if iris:
        plt.xlabel("Petal length", fontsize=14)
        plt.ylabel("Petal width", fontsize=14)
    else:
        plt.xlabel(r"$x_1$", fontsize=18)
        plt.ylabel(r"$x_2$", fontsize=18, rotation=0)
    if legend:
        plt.legend(loc="lower right", fontsize=14)

plt.figure(figsize=(8, 4))
plot_decision_boundary(tree_clf, X, y)
plt.plot([2.45, 2.45], [0, 3], "k-", linewidth=2) # 分割线
plt.plot([2.45, 7.5], [1.75, 1.75], "k--", linewidth=2) 
plt.plot([4.95, 4.95], [0, 1.75], "k:", linewidth=2)
plt.plot([4.85, 4.85], [1.75, 3], "k:", linewidth=2)
plt.text(1.40, 1.0, "Depth=0", fontsize=15)
plt.text(3.2, 1.80, "Depth=1", fontsize=13)
plt.text(4.05, 0.5, "(Depth=2)", fontsize=11)
plt.title('Decision Tree decision boundaries')

plt.show()
```

<img src="../typora_image/image-20220913200751889.png" alt="image-20220913200751889" style="zoom:67%;" />

## 概率估计

*  通过输入特征值，获取概率值或预测值

```python
# 预测概率值
tree_clf.predict_proba([[5,1.5]]) # 分别返回属于三个类别的概率
# array([[0.        , 0.90740741, 0.09259259]])

# 预测值
tree_clf.predict([[5,1.5]])
# array([1])
```

> 输入数据为：花瓣长5厘米，宽1.5厘米的花。
>
> 相应的叶节点是深度为2的左节点，因此决策树应输出以下概率：
>
> * Iris-Setosa 为 0％（0/54），
> * Iris-Versicolor 为 90.7％（49/54），
> * Iris-Virginica 为 9.3％（5/54）。
>
> 最终预测值为1分类。

## 决策树中的正则化

* DecisionTreeClassifier参数：

> min_samples_split：节点在分割之前最小样本数
>
> min_samples_leaf：叶子节点最小样本数
>
> max_leaf_nodes：叶子节点的最多个数（$n_0$）
>
> max_features：在每个节点处评估用于拆分的最大特征数
>
> max_depth：树最大的深度

* 对比实验

```python
# 导入数据集
from sklearn.datasets import make_moons
X,y = make_moons(n_samples=100,noise=0.25,random_state=53)
tree_clf1 = DecisionTreeClassifier(random_state=42)
tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4,random_state=42)
# 训练
tree_clf1.fit(X,y)
tree_clf2.fit(X,y)


plt.figure(figsize=(12,4))
plt.subplot(121)
plot_decision_boundary(tree_clf1,X,y,axes=[-1.5,2.5,-1,1.5],iris=False)
plt.title('No restriction')

plt.subplot(122)
plot_decision_boundary(tree_clf2,X,y,axes=[-1.5,2.5,-1,1.5],iris=False)
plt.title('min_samples_leaf=4')
```

<img src="../typora_image/image-20220917111610256.png" alt="image-20220917111610256" style="zoom:80%;" />

> 左图不做限制时，决策边界较为复杂，且右下角黄点，明显的形成了过拟合；
>
> 而限制了叶子结点最小样本数后，效果改善。

## 对数据的敏感

* 将数据集进行旋转之后的效果

```python
# 构造数据集
np.random.seed(6)
Xs = np.random.rand(100,2) - 0.5
ys = (Xs[:,0] > 0).astype(np.float32) * 2

# 旋转角度
angle = np.pi/4
# 旋转矩阵
rotation_matrix = np.array([[np.cos(angle),-np.sin(angle)],[np.sin(angle),np.cos(angle)]])
# 旋转数据集
Xsr = Xs.dot(rotation_matrix)

# 原始数据集
tree_clf_s = DecisionTreeClassifier(random_state=42)
tree_clf_s.fit(Xs,ys)
# 旋转后数据集
tree_clf_sr = DecisionTreeClassifier(random_state=42)
tree_clf_sr.fit(Xsr,ys)

# 画图
plt.figure(figsize=(11,4))
plt.subplot(121)
plot_decision_boundary(tree_clf_s,Xs,ys,axes=[-0.7,0.7,-0.7,0.7],iris=False)
plt.title('Sensitivity to training set rotation')

plt.subplot(122)
plot_decision_boundary(tree_clf_sr,Xsr,ys,axes=[-0.7,0.7,-0.7,0.7],iris=False)
plt.title('Sensitivity to training set rotation')

plt.show()
```

<img src="../typora_image/image-20220917120010526.png" alt="image-20220917120010526" style="zoom:67%;" />

> 将数据集进行旋转之后，决策边界也会发生改变。

## 回归任务

* 决策树也能做回归的任务

```python
# 构造数据集
np.random.seed(42)
m = 200
X = np.random.rand(m,1)
y = 4*(X-0.5)**2
y = y + np.random.randn(m,1)/10 # 高斯，数据分布抖动
```

```python
# 导入决策树回归模型
from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor(max_depth=2)
tree_reg.fit(X,y)
```

```python
export_graphviz(
    tree_reg,
    out_file=("regression_tree.dot"),
    feature_names=["x1"],
    rounded = True,
    filled = True
)
```

> `dot -T png regression_tree.dot -o regreesion_tree.png`

```python
# 图片展示
from IPython.display import Image
Image(filename="regreesion_tree.png",width=600,height=600)
```

<img src="../typora_image/image-20220917121657658.png" alt="image-20220917121657658" style="zoom:80%;" />

> 在进行回归任务时，可以选择参数来进行分裂，一般情况下都以均方误差，通过比较与均值的距离来进行树的分裂；
>
> 且为二叉树，所用模型为回归树（CART）。

* 回归任务中，树的深度对结果的影响

```python
from sklearn.tree import DecisionTreeRegressor

# 构造两个树模型，其深度不同
tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)
tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

# 预测值画图
def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel="$y$"):
    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)
    y_pred = tree_reg.predict(x1)
    plt.axis(axes)
    plt.xlabel("$x_1$", fontsize=18)
    if ylabel:
        plt.ylabel(ylabel, fontsize=18, rotation=0)
    plt.plot(X, y, "b.")
    plt.plot(x1, y_pred, "r.-", linewidth=2, label=r"$\hat{y}$")

plt.figure(figsize=(11, 4))
plt.subplot(121)

plot_regression_predictions(tree_reg1, X, y)
for split, style in ((0.1973, "k-"), (0.0917, "k--"), (0.7718, "k--")):
    plt.plot([split, split], [-0.2, 1], style, linewidth=2)
plt.text(0.21, 0.65, "Depth=0", fontsize=15)
plt.text(0.01, 0.2, "Depth=1", fontsize=13)
plt.text(0.65, 0.8, "Depth=1", fontsize=13)
plt.legend(loc="upper center", fontsize=18)
plt.title("max_depth=2", fontsize=14)

plt.subplot(122)

plot_regression_predictions(tree_reg2, X, y, ylabel=None)
for split, style in ((0.1973, "k-"), (0.0917, "k--"), (0.7718, "k--")):
    plt.plot([split, split], [-0.2, 1], style, linewidth=2)
for split in (0.0458, 0.1298, 0.2873, 0.9040):
    plt.plot([split, split], [-0.2, 1], "k:", linewidth=1)
plt.text(0.3, 0.5, "Depth=2", fontsize=13)
plt.title("max_depth=3", fontsize=14)

plt.show()
```

<img src="../typora_image/image-20220917122148121.png" alt="image-20220917122148121" style="zoom:80%;" />

> 树的深度越高，切分的越细。

* 叶子结点最小样本个数的不同

```python
# 叶子结点最小样本个数的不同
tree_reg1 = DecisionTreeRegressor(random_state=42)
tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

x1 = np.linspace(0, 1, 500).reshape(-1, 1)
y_pred1 = tree_reg1.predict(x1)
y_pred2 = tree_reg2.predict(x1)

plt.figure(figsize=(11, 4))

plt.subplot(121)
plt.plot(X, y, "b.")
plt.plot(x1, y_pred1, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([0, 1, -0.2, 1.1])
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", fontsize=18, rotation=0)
plt.legend(loc="upper center", fontsize=18)
plt.title("No restrictions", fontsize=14)

plt.subplot(122)
plt.plot(X, y, "b.")
plt.plot(x1, y_pred2, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([0, 1, -0.2, 1.1])
plt.xlabel("$x_1$", fontsize=18)
plt.title("min_samples_leaf={}".format(tree_reg2.min_samples_leaf), fontsize=14)

plt.show()
```

<img src="../typora_image/image-20220917122403584.png" alt="image-20220917122403584" style="zoom:80%;" />

> 左图未限制时，会产生过拟合；而右图作出了限制后，有效的避免了这个问题。
>
> 这就是用决策树解决回归问题。

# 集成算法

* 目的：让机器学习效果更好，单个不行，多个并行一起。
* Bagging：训练多个分类器取平均 $f(x) = \frac{1}{M} \sum^{M}_{m=1}f_m(x)$ 
* Boosting：从弱学习器开始加强，通过加权进行训练。（提升算法）

> 加入一棵树，要比原来强。

* Stacking：聚合多个分类或回归模型（可以分阶段来做）。

## 随机森林

* Bootstrap aggregation（并行训练一堆分类器）
* 随机：选取随机数据、随机特征。

<img src="../typora_image/image-20220918163644435.png" alt="image-20220918163644435" style="zoom:67%;" />

> 通过二重随机性，即数据和特征都是随机的，构造出来的每棵树基本都会不一样，结果也会不一样。

* 森林：很多个决策树并行放在一起。

> 分类任务中，根据每棵树的结果，以结果相同最多的作为最终结果。
>
> 回归任务中，将每棵树进行求和，再求平均作为最终结果。

* 优势

> * 能够处理很高维度（feature很多）的数据，并且不用做特征选择。
>
> * 在训练完后，能够给出哪些特征比较重要。
>
>   * 如何选出重要特征？可以通过对比实验：
>
>     > 如模型1选取特征ABCD，模型2选取特征AB'CD，然后比较准确率。（B'是打乱或者加入异常值的数据）
>     >
>     > 模型1.err ≈ 模型2.err，则B特征不重要；
>     >
>     > 模型1.err < 模型2.err，则B特征重要。
>
> * 容易做成并行化方法，速度比较快。
>
> * **可以进行可视化展示，利与分析。**

* 理论上越多的树效果会越好，但实际上超过一定数量后效果浮动不会发生太大变化。

<img src="../typora_image/image-20220918203854918.png" alt="image-20220918203854918" style="zoom: 67%;" />

## 提升算法（Boosting）

$$
F_m(x) = F_{m-1}(x) + argmin_h\sum^{n}_{i=1}L(y_i,F_{m-1}(x_i)+h(x_i))
$$

> 前一次的树结果+当前一次的树结果；加入的一棵树要比原来的强。（判断损失是否下降）

* 典型代表：AdaBoost，Xgboost

> AdaBoost会根据前一次的分类效果调整数据权重。
>
> 解释：如果某一数据在这次分类错误了，那么在下一次就会赋予它更大的权重。
>
> 最终的结果：每个分类器根据自身的准确性来确定各自的权重，再合体。

## Stacking（少用）

* 堆叠：用多个分类器来解决问题。
* 分阶段：第一阶段得出各自结果，第二阶段再用上一阶段结果训练。

## 构建数据集

```python
# 导入数据集
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons

X,y = make_moons(n_samples=500,noise=0.30,random_state=42)
X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)
```

```python
plt.plot(X[:,0][y==0],X[:,1][y==0],'yo',alpha = 0.6)
plt.plot(X[:,0][y==0],X[:,1][y==1],'bs',alpha = 0.6)
plt.show()
```

<img src="../typora_image/image-20220919231127582.png" alt="image-20220919231127582" style="zoom: 80%;" />

## 投票策略：软投票与硬投票

* 硬投票：直接使用类别值，少数服从多数。

```python
# 导入随机森林和投票器
from sklearn.ensemble import RandomForestClassifier,VotingClassifier
# 导入其他分类器
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# 实例化
log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()

voting_clf = VotingClassifier(estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],voting='hard') # 传入分类器，list结构；投票方式，当前为硬

# 训练
voting_clf.fit(X_train,y_train)
```

<img src="../typora_image/image-20220919232518394.png" alt="image-20220919232518394" style="zoom:80%;" />

```python
# 导入准确率作为评估标准
from sklearn.metrics import accuracy_score

for clf in (log_clf,rnd_clf,svm_clf,voting_clf):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test) # 预测值
    # 打印单个分类器的准确率
    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))
```

```python
LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.896
VotingClassifier 0.904
```

> 随机森林通过硬投票后，提升到了0.904，比之前单个分类器效果都要好。

* 软投票：各自分类器的概率值进行加权平均。

```python
# 实例化
log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC(probability=True) # 可获取概率值

voting_clf = VotingClassifier(estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],voting='soft') 
```

> 软投票只需要将voting改为soft，打印准确率结果为：

```python
LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.896
VotingClassifier 0.92
```

> 软投票的效果要比硬投票效果好。
>
> 要进行软投票的前提是分类器都需要得到概率值。

## Bagging策略

- 首先对训练集进行多次采样，保证每次得到的采样数据都是不同的。
- 对采样数据分别训练多个模型，例如树模型。
- 预测时需要得到所有模型结果再进行集成。

<img src="../typora_image/image-20220920185835721.png" alt="image-20220920185835721" style="zoom: 80%;" />

* 对比实验，Bagging和决策树

```python
# 导入bagging和决策树
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(DecisionTreeClassifier(), # 分类器
                  n_estimators=500, # 树的数量
                 max_samples=100,# 最大样本数
                 bootstrap = True,# 随机采样
                 n_jobs = -1, # 多线程  
                 random_state = 42
                ) 
# 训练
bag_clf.fit(X_train,y_train)
# 预测
y_pred = bag_clf.predict(X_test)
# 准确率
accuracy_score(y_test,y_pred)
# 0.904
```

```python
# 对比实验，单用决策树
ree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train,y_train)
y_pred_tree = tree_clf.predict(X_test)
accuracy_score(y_test,y_pred_tree)
# 0.856
```

> 使用Bagging算法的评估为0.904，明显优于单用决策树的评估0.856。

## 决策边界

- 集成与传统方法对比

```python
from matplotlib.colors import ListedColormap

# 画图：决策边界
def plot_decasion_boundary(clf,X,y,axes=[-1.5,2.5,-1,1.5],alpha=0.5,contour = True): # axes:范围；alpha:透明度；contour:轮廓
    # 取值范围
    x1s = np.linspace(axes[0],axes[1],100)
    x2s = np.linspace(axes[2],axes[3],100)
    x1,x2 = np.meshgrid(x1s,x2s) # 生成网格点
    X_new = np.c_[x1.ravel(),x2.ravel()] # 拉长，拼接
    y_pred = clf.predict(X_new).reshape(x1.shape) # 预测值,z
    
    # 用不同颜色表达分类边界
    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])
    plt.contourf(x1,x2,y_pred,cmap = custom_cmap,alpha=0.3)
    
    # 等高线
    if contour:
        # 生成颜色
        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])
        plt.contour(x1,x2,y_pred,cmap = custom_cmap2,alpha=0.8)
    # 数据集
    plt.plot(X[:,0][y==0],X[:,1][y==0],'yo',alpha = 0.6)
    plt.plot(X[:,0][y==0],X[:,1][y==1],'bs',alpha = 0.6) 
    plt.axis(axes)
    plt.xlabel('x1')
    plt.ylabel('x2')
```

```python
# 对比实验
plt.figure(figsize=(12,5))
plt.subplot(121)
plot_decasion_boundary(tree_clf,X,y)
plt.title('Decision Tree')

plt.subplot(122)
plot_decasion_boundary(bag_clf,X,y)
plt.title('Decision Tree With Bagging ')
```

<img src="../typora_image/image-20220920200350175.png" alt="image-20220920200350175" style="zoom: 80%;" />

> 左图为单一决策树算法；右图为集成算法，决策边界更平滑，过拟合风险更低。

## OOB策略

* m个训练样本会通过bootstrap (有放回的随机抽样) 的抽样方式进行T次抽样每次抽样产生样本数为m的采样集，进入到并行的T个决策树中。这样有放回的抽样方式会导致有部分训练集中的样本(约36.8%)未进入决策树的采样集中，而这部分未被采集的的样本就是袋外数据oob。

* 而这个袋外数据就可以用来检测模型的泛化能力，和交叉验证类似。可以理解成从train datasets 中分出来的validation datasets。

```python
bag_clf = BaggingClassifier(DecisionTreeClassifier(), # 分类器
                  n_estimators=500, # 树的数量
                 max_samples=100,# 最大样本数
                 bootstrap = True,# 随机采样
                 n_jobs = -1, # 多线程  
                 random_state = 42,
                 oob_score = True # 使用oob
                ) 
bag_clf.fit(X_train,y_train)
# 验证集评估
bag_clf.oob_score_
# 0.9253333333333333
```

```python
y_pred = bag_clf.predict(X_test)
accuracy_score(y_test,y_pred)
# 0.904
```

> 一般情况下，验证集会比测试集的得分高一点。

```python
# 属于各个类别的概率值
bag_clf.oob_decision_function_
```

## 特征重要性热度图展示

* 一个数据集中往往有成百上千个特征，如何在其中选择比结果影响最大的那几个特征，以此来缩减建立模型时的特征数是我们比较关心的问题。
* 用随机森林进行特征重要性评估的思想其实很简单，说白了就是看看每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。

```python
# 导入数据集
from sklearn.datasets import load_iris
iris = load_iris()
rf_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)
rf_clf.fit(iris['data'],iris['target'])
# 打印特征重要性
for name,score in zip(iris['feature_names'],rf_clf.feature_importances_):
    print(name,score)
```

```python
sepal length (cm) 0.11105536416721994
sepal width (cm) 0.02319505364393038
petal length (cm) 0.44036215067701534
petal width (cm) 0.42538743151183406
```

* 用热度图来表示重要性

```python
from sklearn.datasets import fetch_mldata
mnist = fetch_mldata('MNIST original')
```

```python
rf_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)
rf_clf.fit(mnist['data'],mnist['target'])

rf_clf.feature_importances_.shape # 把每个像素当成一个特征
# (784,)
```

```python
# 热度图
def plot_digit(data):
    image = data.reshape(28,28)
    plt.imshow(image,cmap=matplotlib.cm.hot)
    plt.axis('off') # 不显示坐标轴
```

```python
plot_digit(rf_clf.feature_importances_)
char = plt.colorbar(ticks = [rf_clf.feature_importances_.min(),rf_clf.feature_importances_.max()]) # 深浅度
char.ax.set_yticklabels(['Not important','Very important'])
```

<img src="../typora_image/image-20220920212731370.png" alt="image-20220920212731370" style="zoom:80%;" />

> 热度图中，颜色越浅表示特征越重要。

## Adaboost算法概述

* Boosting算法的工作机制

  > 首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。
  >
  > 然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

<img src="../typora_image/image-20220921223157145.png" alt="image-20220921223157145" style="zoom: 67%;" />

* 以SVM分类器为例演示AdaBoost的基本策略

```python
m = len(X_train) # 样本数量

# 不同调节程度对结果的影响
plt.figure(figsize=(14,5))
for subplot,learning_rate in ((121,1),(122,0.5)):
    sample_weights = np.ones(m) # 初始化权重
    plt.subplot(subplot)
    for i in range(5):
        svm_clf = SVC(kernel='rbf',C=0.05,random_state=42) 
        svm_clf.fit(X_train,y_train,sample_weight=sample_weights) # 加入权重项
        y_pred = svm_clf.predict(X_train) # 预测值
        # 改变权重
        sample_weights[y_pred != y_train] *= (1+learning_rate) # 分类错误，加重权重
        # 决策边界
        plot_decasion_boundary(svm_clf,X,y,alpha=0.2)
        plt.title('learning_rate = {}'.format(learning_rate))
    if subplot == 121:
        plt.text(-0.7, -0.65, "1", fontsize=14)
        plt.text(-0.6, -0.10, "2", fontsize=14)
        plt.text(-0.5,  0.10, "3", fontsize=14)
        plt.text(-0.4,  0.55, "4", fontsize=14)
        plt.text(-0.3,  0.90, "5", fontsize=14)
plt.show()
```

<img src="../typora_image/image-20220921224735912.png" alt="image-20220921224735912" style="zoom: 67%;" />

> 左图中，学习率较大时，每一次给的权重也会变大；

* 可以直接导入AdaBoost模型

```python
# 直接导入AdaBoost模型
from sklearn.ensemble import AdaBoostClassifier
ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                    n_estimators=200,
                   learning_rate=0.5,
                   random_state=42                  
)

ada_clf.fit(X_train,y_train)
plot_decasion_boundary(ada_clf,X,y)
```

<img src="../typora_image/image-20220921224839723.png" alt="image-20220921224839723" style="zoom: 80%;" />

## GBDT 梯度提升决策树

* 将上一次做得不好的结果进行训练。

```python
# 构造数据集
np.random.seed(42)
X = np.random.rand(100,1) - 0.5
y = 3*X[:,0]**2 + 0.05 * np.random.randn(100)

# 导入回归树
from sklearn.tree import DecisionTreeRegressor
tree_reg1 = DecisionTreeRegressor(max_depth=2)
tree_reg1.fit(X,y)

# 第一次预测的数据集误差作为下一次的训练集
y2 = y - tree_reg1.predict(X)

tree_reg2 = DecisionTreeRegressor(max_depth=2)
tree_reg2.fit(X,y2)

# 第二次预测的数据集误差作为下一次的训练集
y3 = y2 - tree_reg2.predict(X)

tree_reg3 = DecisionTreeRegressor(max_depth=2)
tree_reg3.fit(X,y3)

# 测试集
X_new = np.array([[0.8]])
# 用训练集测试每个模型之后，进行求和，即为最终预测值
y_pred = sum(tree.predict(X_new) for tree in (tree_reg1,tree_reg2,tree_reg3))
y_pred
# array([0.75026781])
```

> 最终得到的预测值结果为0.75026781，是通过将每个模型的预测值进行求和得到的。

> 在sklearn中可以直接导入GBDT模型：
>
> 1. GBDT
> 2. XgBoost（建议）
> 3. lightgbm（建议）

## 集成参数对比分析

* 对上述三个树模型可视化展示

```python
def plot_predictions(regressors, X, y, axes, label=None, style="r-", data_style="b.", data_label=None):
    x1 = np.linspace(axes[0], axes[1], 500) # 测试集
    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors) # 预测值
    plt.plot(X[:, 0], y, data_style, label=data_label)
    plt.plot(x1, y_pred, style, linewidth=2, label=label)
    if label or data_label:
        plt.legend(loc="upper center", fontsize=16)
    plt.axis(axes)

plt.figure(figsize=(11,11))

plt.subplot(321)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h_1(x_1)$", style="g-", data_label="Training set")
plt.ylabel("$y$", fontsize=16, rotation=0)
plt.title("Residuals and tree predictions", fontsize=16)

plt.subplot(322)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1)$", data_label="Training set")
plt.ylabel("$y$", fontsize=16, rotation=0)
plt.title("Ensemble predictions", fontsize=16)

plt.subplot(323)
plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label="$h_2(x_1)$", style="g-", data_style="k+", data_label="Residuals")
plt.ylabel("$y - h_1(x_1)$", fontsize=16)

plt.subplot(324)
plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1) + h_2(x_1)$")
plt.ylabel("$y$", fontsize=16, rotation=0)

plt.subplot(325)
plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label="$h_3(x_1)$", style="g-", data_style="k+")
plt.ylabel("$y - h_1(x_1) - h_2(x_1)$", fontsize=16)
plt.xlabel("$x_1$", fontsize=16)

plt.subplot(326)
plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$")
plt.xlabel("$x_1$", fontsize=16)
plt.ylabel("$y$", fontsize=16, rotation=0)

plt.show()
```

![image-20220922191604006](../typora_image/image-20220922191604006.png)

> 左列图中，表示残差的高低，越接近零，残差越小；反之，绿线向上或向下的幅度为残差较大。
>
> 右列图中，表示拟合的程度。
>
> 可以看出，在多次对模型进行集成后，可以降低残差、提高拟合的效果。

* 用sklearn自带模型进行对比实验

> 建立三个模型，不同参数。

```python
# 导入GBDT模型
from sklearn.ensemble import GradientBoostingRegressor

gbdt = GradientBoostingRegressor(max_depth=2, # 最大深度
                          n_estimators = 3, # 树的数量
                          learning_rate = 1.0 , # 相当于每次树的权重
                          random_state = 41
)
gbdt.fit(X,y)

gbdt_slow_1 = GradientBoostingRegressor(max_depth=2, 
                          n_estimators = 3, 
                          learning_rate = 0.1 , # 学习率变小
                          random_state = 41
)
gbdt_slow_1.fit(X,y)

gbdt_slow_2 = GradientBoostingRegressor(max_depth=2, 
                          n_estimators = 200, # 树的数量变多
                          learning_rate = 0.1 , 
                          random_state = 41
)
gbdt_slow_2.fit(X,y)
```

```python
# 画图
plt.figure(figsize=(11,4))
plt.subplot(121)
plot_predictions([gbdt],X,y,axes=[-0.5,0.5,-0.1,0.8],label = 'Ensemble predictions')
plt.title('learning_rate={},n_estimators={}'.format(gbdt.learning_rate,gbdt.n_estimators))

plt.subplot(122)
plot_predictions([gbdt_slow_1],X,y,axes=[-0.5,0.5,-0.1,0.8],label = 'Ensemble predictions')
plt.title('learning_rate={},n_estimators={}'.format(gbdt_slow_1.learning_rate,gbdt_slow_1.n_estimators))
```

<img src="../typora_image/image-20220922193412599.png" alt="image-20220922193412599" style="zoom:80%;" />

> 右图中，学习率变小了，但树的数量也小，其效果就很差。
>
> 所以，学习率变小时，n_estimators应设置较大（相当于迭代次数），所以有：

```python
# 画图
plt.figure(figsize=(11,4))
plt.subplot(121)
plot_predictions([gbdt_slow_2],X,y,axes=[-0.5,0.5,-0.1,0.8],label = 'Ensemble predictions')
plt.title('learning_rate={},n_estimators={}'.format(gbdt_slow_2.learning_rate,gbdt_slow_2.n_estimators))

plt.subplot(122)
plot_predictions([gbdt_slow_1],X,y,axes=[-0.5,0.5,-0.1,0.8],label = 'Ensemble predictions')
plt.title('learning_rate={},n_estimators={}'.format(gbdt_slow_1.learning_rate,gbdt_slow_1.n_estimators))
```

<img src="../typora_image/image-20220922193642622.png" alt="image-20220922193642622" style="zoom:80%;" />

> 增大了n_estimators参数，拟合效果变好了。

## 提前停止策略

* 一般情况下，损失会根据迭代次数的增加而下降，然后到达一个平稳的阶段；但如果出现到达最小损失后进行了一个小幅度的增加，就要考虑在那个位置上停止算法的执行。

```python
from sklearn.metrics import mean_squared_error
# 构造数据集
X_train,X_val,y_train,y_val = train_test_split(X,y,random_state=49)

gbdt = GradientBoostingRegressor(max_depth=2, 
                          n_estimators = 120, 
                          random_state = 42
)
gbdt.fit(X_train,y_train)

# 分阶段预测的均方误差值
errors = [mean_squared_error(y_val,y_pred) for y_pred in gbdt.staged_predict(X_val)]
# 找到最小值，即为最好情况
bst_n_estimators = np.argmin(errors) # 返回的是最小值下标
```

> 知道了最佳迭代次数，直接赋值给n_estimators，然后进行训练：

```python
gbrt_best = GradientBoostingRegressor(max_depth=2, 
                          n_estimators = bst_n_estimators, 
                          random_state = 42
)
gbrt_best.fit(X_train,y_train)

# 最小损失值
min_error = np.min(errors)
min_error
# 0.003009529324545136
```

* 画图

```python
# 画图
plt.figure(figsize=(11,4))
plt.subplot(121)
# 损失值
plt.plot(errors,'b.-') 
plt.plot([bst_n_estimators,bst_n_estimators],[0,min_error],'k--') # 竖线
plt.plot([0,120],[min_error,min_error],'k--')  # 横线
plt.axis([0,120,0,0.01])
plt.title('Val Error')

# 最好的那次
plt.subplot(122)
plot_predictions([gbdt_best],X,y,axes=[-0.5,0.5,-0.1,0.8])
plt.title('Best Model(%d trees)'%bst_n_estimators)
```

<img src="../typora_image/image-20220922202203169.png" alt="image-20220922202203169" style="zoom:80%;" />

> 左图中，虚线所对应位置为损失最小的地方，即最佳得带次数；
>
> 然后根据该次模型，得到右图最佳拟合效果，最佳的拟合参数为n_estimators=55。

* 热启动，在前一阶段的训练结果上继续训练

```python
gbrt = GradientBoostingRegressor(max_depth=2, 
                          random_state = 42,
                            warm_start = True # 热启动，在前一阶段的训练结果上继续训练
)
# 损失值上浮次数
error_going_up = 0 
# 初始化为无穷大
min_val_error = float('inf')

# n_estimator从1到120
for n_estimator in range(1,120):
    gbrt.n_estimators = n_estimator
    gbrt.fit(X_train,y_train)
    y_pred = gbrt.predict(X_val)
    val_error = mean_squared_error(y_val,y_pred)
    # 找到比之前小的损失值
    if val_error < min_val_error:
        min_val_error = val_error # 更新
        error_going_up = 0
    else:
        error_going_up += 1
    if error_going_up == 5: # 超过五次就停止
        break
            
print(gbrt.n_estimators) # 从55次开始，连续5次迭代都没有上升
# 61
```

## Stacking（堆叠集成）

* 第一阶段：选用多个模型进行训练。
* 第二阶段：用上一阶段的结果作为训练集。

```python
# 导入数据集，作10分类任务
from sklearn.datasets import fetch_mldata
from sklearn.model_selection import train_test_split

mnist = fetch_mldata('MNIST original')

# 分割数据集
X_train, X_val, y_train, y_val = train_test_split(
    mnist.data, mnist.target, test_size=10000, random_state=42)
```

```python
# 导入多个分类器
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier

# 第一阶段，分类器各自进行训练
random_forest_clf = RandomForestClassifier(random_state=42)
extra_trees_clf = ExtraTreesClassifier(random_state=42)
svm_clf = LinearSVC(random_state=42)
mlp_clf = MLPClassifier(random_state=42)

estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]

# 训练
for estimator in estimators:
    print('Training the',estimator)
    estimator.fit(X_train,y_train)
```

```python
# 存放预测值
X_val_predictions = np.empty((len(X_val),len(estimators)),dtype=np.float32)

for index, estimator in enumerate(estimators):
    # 存储第一阶段的结果
    X_val_predictions[:,index] = estimator.predict(X_val)

# 得到四个分类器的预测结果
X_val_predictions
```

```python
# 第二阶段，选择随机森林模型
rnd_forest_blender = RandomForestClassifier(n_estimators=200,oob_score=True,random_state=42)

rnd_forest_blender.fit(X_val_predictions,y_val)

# 验证集评估
rnd_forest_blender.oob_score_
# 0.9641
```

# 支持向量机

* 要解决的问题

  * 什么样的决策边界才是最好的？

  * 特征数据本身很难分类怎么办？

  * 计算复杂度、实际应用

## 决策边界推导

* 选出来离样本点区域最远的

![image-20220926152239076](../typora_image/image-20220926152239076.png)

> 当前选择的区域，要与最近样本点来划分，且划分出来的距离是最大的。

* 距离的计算

![image-20220926152635907](../typora_image/image-20220926152635907.png)

> 图中，作 **x** 的垂线即为点到平面的距离。
>
> 设$x',x''$为平面上两点，则有：$w^Tx'=-b,w^Tx''=-b$
>
> 则计算 x 与 x',x'' 的两点之间距离，再投影到dist(x,h)处，即为x垂线的距离，则有：
> $$
> distance(x,b,w) = |\frac{w^T}{||w||}(x-x')| = \frac{1}{||w||}|w^Tx+b|
> $$
> 

* 数据标签定义

> 数据集：（x1,y1),(x2,y2),...,(xn,yn)，有监督。
>
> Y为样本的类别：当X为正类时，Y = +1 ，当X为负类时，  Y = -1
>
> 决策方程：
> $$
> y(x) = w^T\phi(x)+b \\[2px]
> ⟹  
> 
> \begin{cases}
> y(x_i)>0 \leftrightarrow y_i = +1 \\
> y(x_i)<0 \leftrightarrow y_i = -1 \\
> \end{cases} 
> ⟹ y_i ·y(x_i) > 0
> $$

* 目标函数推导

> 通俗解释：找到一条线（w和b），使得离该线最近的点能够最远。
>
> 将点到直线的距离化简：
> $$
> \frac{y_i·(wT·\phi(x_i)+b)}{||w||}
> $$
>  

> 优化目标：
> $$
> arg max_{w,b}\Bigg\{\frac{1}{||w||}min_i[y_i·(w^T·\phi(x_i)+b)]\Bigg\}
> $$

> 内层的**min**，是找**i**（样本）中离边界（平面）最近的点；
>
> 外层的**argmax**，找最大的**w,b**使得距离（决策边界）最宽。

> 放缩变换：对于决策方程 $y(x)$ 可以通过放缩使得结果值$Y>=1$  ，则有：
> $$
> y_i·(w^T·\phi(x_i)+b) ≥ 1
> $$
> 即恒大于0，则求得的**min**为1，化简后可得**目标函数**为：
> $$
> arg max_{w,b} \frac{1}{||w||}
> $$

* 拉格朗日乘数法求解（条件下极值）

> 常规套路：将求解极大值问题转为求极小值问题 → $min_{w,b} \frac{1}{2}w^2$
>
> 则有在$y_i·(wT·\phi(x_i)+b) ≥ 1$ ，求：
> $$
> L(w,b,a)  = \frac{1}{2}||w||^2-\sum_{i=1}^nα_i(y_i·(w^T·\phi(x_i)+b)- 1)
> $$

* SVM求解

> 由于**对偶性质**，分别对**w**和**b**求偏导，得到两个条件：
> $$
> min_{w,b}\,\,max_{α}L(w,b,a) ->max_α\,\,min_{w,b}L(w,b,a)
> $$
>
> $$
> \frac{\partial L}{\partial w}=0→w=\sum_{i=1}^{n}α_iy_i\phi(x_n) \\[2ex]
> \frac{\partial L}{\partial b}=0→0=\sum_{i=1}^{n}α_iy_i
> $$
>
> 将$w=\sum_{i=1}^{n}α_iy_i\phi(x_n)$，$0=\sum_{i=1}^{n}α_iy_i$代入有：
> $$
> L(w,b,a)  = \frac{1}{2}||w||^2-\sum_{i=1}^nα_i(y_i·(w^T·\phi(x_i)+b)- 1) \\[2ex]
> =\sum_{i=1}^{n}a_i-\frac{1}{2}\sum_{i=1,j=1}^{n}α_iα_jy_iy_j\phi^T(x_i)\phi(x_j)
> $$
> 即完成了第一步求解，将**w,b**全部转换为了**α**。

> 对α求极大值转为求极小值，即将式子添加负号：
> $$
> min_α[\frac{1}{2}\sum_{i=1,j=1}^{n}α_iα_jy_iy_j\phi^T(x_i)\phi(x_j)-\sum_{i=1}^{n}a_i]
> $$
> 条件：$\sum_{i=1}^{n}a_iy_i=0，a_i≥0$

* SVM求解实例

> 数据：3个点，其中正例x1(3,3) ，x2(4,3)，负例x3(1,1)
>
> 求解：$\frac{1}{2}\sum_{i=1,j=1}^{n}α_iα_jy_iy_j\phi^T(x_i)\phi(x_j)-\sum_{i=1}^{n}a_i$
>
> 其中，由x1，x2为正例（+1），x3为负例（-1），故有约束条件：$α_1+α_2-α_3=0，α_i≥0(i=1,2,3)$
>
> 代入求解式有：
>
> <img src="../typora_image/image-20220927203001375.png" alt="image-20220927203001375" style="zoom:80%;" />

> 对$α_1,α_2$求偏导值，令其等于0：$α_1=1.5，α_2=-1$
>
> 但由于条件$α_i≥0$，所以上述解不成立，则考虑解应该在**边界**上；
>
> 其中，令$α_2=0,有α_1=0.25，且可推出α_3=0.25$，则得出坐标$(0.25,0,0.25)$ 

> 将α结果代入：
> $$
> w=\sum_{i=1}^{n}a_iy_i\phi(x_n) \\[2px]
> b = y_i-\sum_{i=1}^na_iy_i(x_ix_j)
> $$
> 有 $w=(\frac{1}{2},\frac{1}{2}),b=-2$
>
> 平面方程：$0.5x_1+0.5_x2-2=0$

* 支持向量：真正发挥作用的数据点，α值不为0的点

![image-20220927205504884](../typora_image/image-20220927205504884.png)

> 左图样本点为60个，但决策边界只考虑其中的三个点；
>
> 而当样本点增加到了120，只要不影响**边界点**，决策边界依然不会改变。

* 软间隔（soft-margin）

> 有时数据中有一些噪音点，如果将它们考虑在内，则决策边界效果会降低。
>
> ![image-20220927210226419](../typora_image/image-20220927210226419.png)
>
> 这时，需要引入松弛因子 $\xi$ ：
> $$
> y_i(w·x_i+b)≥1-\xi_i
> $$
> 得到新的目标函数：
> $$
> min \frac{1}{2}||w||^2+C\sum_{i=0}^n\xi_i
> $$
> 上述的值应当越小越好，有两种情况：
>
> * 当C趋近于很大时：意味着分类严格不能有错误；
> * 当C趋近于很小时：意味着可以有更大的错误容忍。
>
> C是在建立模型时要调试的一个参数。

* 核函数

> * 低维不可分问题
>
> 核变换：低维数据不可分，则将它映射到高维
>
> ![image-20220928154432072](../typora_image/image-20220928154432072.png)
>
> 如图，二维数据中不好表现，则将它映射到三维，决策边界就很明显了。
>
> 思想：对样本点进行特征变换，再计算内积，最后就映射到高维空间中。

> 高斯核函数：
> $$
> K(X,Y)=e^{-\frac{{||X-Y||}^2}{2\sigma^2} }
> $$
> ![image-20220928155933534](../typora_image/image-20220928155933534.png)
>
> 左图中，线性无法解决该分类问题；
>
> 而使用高斯核函数，形成非线性决策边界，可以将分类做得很好。

## 决策边界可视化展示

* 与其他模型比较决策边界

```python
# 导入支持向量机模型，数据集
from sklearn.svm import SVC
from sklearn import datasets

# 数据集准备
iris = datasets.load_iris()
X = iris['data'][:,(2,3)] # 选择两维特征数据
y = iris['target']
setosa_or_versicolor = (y==0)|(y==1) # 取出两个类别的target值
X = X[setosa_or_versicolor]
y = y[setosa_or_versicolor]

# 一般的模型
x0 = np.linspace(0,5.5,200)
pred_1 = 5*x0 - 20
pred_2 = x0-1.8
pred_3 = 0.1 * x0 + 0.5
```

```python
# 分类器
svm_clf = SVC(kernel='linear',C=float('inf')) # kernel:使用的核函数；C:松弛因子；
svm_clf.fit(X,y)
```

> `kernel`：使用的核函数。
>
> `C`：松弛因子；当前用的无限大，说明分类严格不能出错。

* 决策边界画图

```python
# 画图：决策边界
def plot_svc_decision_boundary(svm_clf,xmin,xmax,sv=True):
    # 根据公式：x0w0+x1w1+b=0
    w = svm_clf.coef_[0] # 权重
    b = svm_clf.intercept_[0] # 偏置
    x0 = np.linspace(xmin,xmax,200)
    # 决策方程
    decision_boundary = -w[0]/w[1] * x0 - b/w[1]  
    # 边界距离
    margin = 1/w[1] 
    # 两条决策线，分别在上方和下方
    gutter_up = decision_boundary + margin
    gutter_down = decision_boundary - margin
    # 支持向量
    if sv:
        # 边界点
        svs = svm_clf.support_vectors_ 
        # 标注边界点
        plt.scatter(svs[:,0],svs[:,1],s=180,facecolors='#FFAAAA')
    # 三条决策线
    plt.plot(x0,decision_boundary,'k-',linewidth=2)
    plt.plot(x0,gutter_up,'k--',linewidth=2)
    plt.plot(x0,gutter_down,'k--',linewidth=2)
```

> `margin`：表示两个边界点中间的位置，可以通过次位置得出两条决策线。

```python
plt.figure(figsize=(14,4))
# 其他模型
plt.subplot(121)
plt.plot(X[:,0][y==1],X[:,1][y==1],'bs')
plt.plot(X[:,0][y==0],X[:,1][y==0],'bs')
plt.plot(x0,pred_1,'g--',linewidth=2)
plt.plot(x0,pred_2,'m-',linewidth=2)
plt.plot(x0,pred_3,'r-',linewidth=2)
plt.plot(x0,)
plt.axis([0,5.5,0,2])
plt.title('Other model')

# 支持向量机
plt.subplot(122)
# 决策边界
plot_svc_decision_boundary(svm_clf,0,5.5,)
# 数据点
plt.plot(X[:,0][y==1],X[:,1][y==1],'bs')
plt.plot(X[:,0][y==0],X[:,1][y==0],'bs')
plt.axis([0,5.5,0,2])
plt.title('Support vector machine')
```

![image-20220929200151281](../typora_image/image-20220929200151281.png)

## 数据标准化的影响

* 在拿到数据集后，最好将数据做标准化操作

![image-20220929201917291](../typora_image/image-20220929201917291.png)

## 软间隔

* 如果不加入软间隔会遇到哪些问题？

![image-20220929202127354](../typora_image/image-20220929202127354.png)

> 左图出现了一个离群点，是无法画出决策边界的；
>
> 右图中出现了一个较远的边界点，使边界距离变小了，同样也不好。
>
> 即当出现一些异常点时，支持向量机的决策效果就变得不好了。

* 可以使用超参数**C**控制软间隔程度

```python
import numpy as np
from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

# 导入数据集
iris = datasets.load_iris()
X = iris['data'][:,(2,3)] # 取特征：petal length,petal width
y = (iris['target'] == 2).astype(np.float64) # 两个类别

# 建立管道：标准化 --> 模型
svm_clf = Pipeline((
    ('std',StandardScaler()),
    ('linear_svc',LinearSVC(C=1)),
))
svm_clf.fit(X,y)

# 预测
svm_clf.predict([[5.5,1.7]])
# array([1.])
```

* 对比不同C值所带来的的效果差异

```python
scaler = StandardScaler()
svm_clf1 = LinearSVC(C=1,random_state=42)
svm_clf2 = LinearSVC(C=100,random_state=42)

scaled_svm_clf1 = Pipeline((
    ('std',scaler),
    ('linear_svc',svm_clf1),
))

scaled_svm_clf2 = Pipeline((
    ('std',scaler),
    ('linear_svc',svm_clf2),
))

scaled_svm_clf1.fit(X,y)
scaled_svm_clf2.fit(X,y)
```

```python
# 因为标准化，所以需要恢复数据原始值
b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])
b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])
w1 = svm_clf1.coef_[0] / scaler.scale_
w2 = svm_clf2.coef_[0] / scaler.scale_
svm_clf1.intercept_ = np.array([b1])
svm_clf2.intercept_ = np.array([b2])
svm_clf1.coef_ = np.array([w1])
svm_clf2.coef_ = np.array([w2])
```

```python
# 画图
plt.figure(figsize=(14,4.2))
plt.subplot(121)
# 数据集
plt.plot(X[:,0][y==1],X[:,1][y==1],'g^',label='Iris-Virginica')
plt.plot(X[:,0][y==0],X[:,1][y==0],'bs',label='Iris-Versicolor')
# 决策边界
plot_svc_decision_boundary(svm_clf1,4,6,sv=False)
plt.xlabel('Petal length',fontsize=14)
plt.ylabel('Petal width',fontsize=14)
plt.legend(loc = 'upper left',fontsize=14)
plt.title("$C = {}$".format(svm_clf1.C),fontsize=16)
plt.axis([4,6,0.8,2.8])

plt.subplot(122)
# 数据集
plt.plot(X[:,0][y==1],X[:,1][y==1],'g^',label='Iris-Virginica')
plt.plot(X[:,0][y==0],X[:,1][y==0],'bs',label='Iris-Versicolor')
# 决策边界
plot_svc_decision_boundary(svm_clf2,4,6,sv=False)
plt.xlabel('Petal length',fontsize=14)
plt.ylabel('Petal width',fontsize=14)
plt.legend(loc = 'upper left',fontsize=14)
plt.title("$C = {}$".format(svm_clf2.C),fontsize=16)
plt.axis([4,6,0.8,2.8])
```

<img src="../typora_image/image-20220930101720983.png" alt="image-20220930101720983" style="zoom:80%;" />

> 在左侧，使用较低的C值，间隔要大得多，但很多实例最终会出现在间隔之内；
>
> 说明分类可以有一定的错误，这是为了减少过拟合的风险。
>
> 在右侧，使用较高的C值，分类器会减少误分类，但最终会有较小的间隔。
>
> 一般情况下，为了避免过拟合，应将C值选择较小，但实际中，需要通过评估来选择C的大小。

## 非线性支持向量机

* 创建一份较难的数据

```python
# 导入数据集
from sklearn.datasets import make_moons
X,y = make_moons(n_samples=100,noise=0.15,random_state=42)

# 画图：数据集
def plot_dataset(X,y,axes):
    plt.plot(X[:,0][y==0],X[:,1][y==0],'bs')
    plt.plot(X[:,0][y==1],X[:,1][y==1],'g^')
    plt.axis(axes)
    plt.grid(True,which='both')
    plt.xlabel(r"$x_1$",fontsize=20)
    plt.ylabel(r"$x_2$",fontsize=20,rotation=0)

plot_dataset(X,y,[-1.5,2.5,-1,1.5])
plt.show()
```

![image-20220930160258229](../typora_image/image-20220930160258229.png)

- 尝试对该数据集进行分类

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

# 管道：构造特征 --> 标准化 --> 模型
polynomial_svm_clf = Pipeline((
    ('poly_features',PolynomialFeatures(degree=3)),
    ('scaler',StandardScaler()),
    ('svm_clf',LinearSVC(C=10,loss='hinge')) # loss:指定损失函数
))
polynomial_svm_clf.fit(X,y)
```

```python
# 画图：决策边界
def plot_predictions(clf,axes):
    x0s = np.linspace(axes[0],axes[1],100)
    x1s = np.linspace(axes[2],axes[3],100)
    x0,x1 = np.meshgrid(x0s,x1s)
    X = np.c_[x0.ravel(),x1.ravel()] 
    y_pred = clf.predict(X).reshape(x0.shape) # 预测值
    plt.contour(x0,x1,y_pred,cmap=plt.cm.brg,alpha=0.2) # 等高线
    
plot_predictions(polynomial_svm_clf,[-1.5,2.5,-1,1.5])
plot_dataset(X,y,[-1.5,2.5,-1,1.5])
```

![image-20220930160352467](../typora_image/image-20220930160352467.png)

> 当前做出的决策边界是曲线的，是用到了构造特征那一步，而模型还是用的线性的。

## SVM中的核技巧

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# poly的对比实验
poly_kernel_svm_clf = Pipeline([
    ('scaler',StandardScaler()),
    ('svm_clf',SVC(kernel='poly',degree=3,coef0=1,C=5)) # 核函数为poly；coef0:偏置项，相当于常数项b
])

poly_kernel_svm_clf.fit(X,y)

ploy100_kernel_svm_clf = Pipeline([
    ('scaler',StandardScaler()),
    ('svm_clf',SVC(kernel='poly',degree=10,coef0=100,C=5))
])
ploy100_kernel_svm_clf.fit(X,y)
```

```python
# 结果展示
plt.figure(figsize=(11,4))

plt.subplot(121)
plot_predictions(poly_kernel_svm_clf,[-1.5,2.5,-1,1.5])
plot_dataset(X,y,[-1.5,2.5,-1,1.5])
plt.title("$d=3,r=1,C=5$",fontsize=18)

plt.subplot(122)
plot_predictions(ploy100_kernel_svm_clf,[-1.5,2.5,-1,1.5])
plot_dataset(X,y,[-1.5,2.5,-1,1.5])
plt.title("$d=10,r=100,C=5$",fontsize=18)

plt.show()
```

![image-20220930165037419](../typora_image/image-20220930165037419.png)

> 左边，`degree`值较小时，做的较为简单一些；
>
> 而较大时，会变得更为复杂。

* 高斯核函数

> 利用相似度来变换特征。将一维数据映射成二维的。
>
> 选择一份一维数据集，并在$x_1=-2$，$x_2=1$处为其添加两个高斯函数。
>
> 然后，将相似函数定义为$γ=0.3$的径向基函数（RBF）：
> $$
> \phiγ(x,l) = e^{-l||x-l||^2}
> $$
> 公式中，$x-l$代表地标的距离；
>
> 例如：$x_1=-1$：它位于距第一个地标距离为1的地方，距第二个地标距离为2.因此，其新特征是$x_2=exp(-0.3*1^2)≈0.74$，$x_3=exp(-0.3*2^2)≈0.30$。
>
> ![image-20221004230948676](../typora_image/image-20221004230948676.png)

```python
# 高斯函数
def gaussian_rbf(x,landmark,gamma):
    return np.exp(-gamma * np.linalg.norm(x - landmark,axis=1)**2)

gamma = 0.1

# 一维数据
x1s = np.linspace(-4.5,4.5,200).reshape(-1,1)
# 通过高斯函数，映射成二维数据
x2s = gaussian_rbf(x1s,-2,gamma) 
x3s = gaussian_rbf(x1s,1,gamma)

# 拼接
XK = np.c_[gaussian_rbf(X1D,-2,gamma),gaussian_rbf(X1D,1,gamma)]
yk = np.array([0,0,1,1,1,1,1,0,0])

# 画图
plt.figure(figsize=(11,4))

plt.subplot(121)
plt.grid(True,which='both')
plt.axhline(y=0,color='k')
plt.scatter(x=[-2,1],y=[0,0],s=150,alpha=0.5,c='red')
plt.plot(X1D[:,0][yk==0],np.zeros(4),'bs')
plt.plot(X1D[:,0][yk==1],np.zeros(5),'g^')
plt.plot(x1s,x2s,'g--')
plt.plot(x1s,x3s,'b:')
plt.gca().get_yaxis().set_ticks([0,0.25,0.5,0.75,1])
plt.xlabel(r"$x_1$",fontsize=20)
plt.ylabel(r"Similarity",fontsize=14)
plt.annotate(r'$\mathbf{}$',
            xy = (X1D[3,0],0),
            xytext = (-0.5,0.20),
            ha = 'center',
            arrowprops=dict(facecolor='black',shrink=0.1),
            fontsize=18,
            )
plt.text(-2,0.9,'$x_2$',ha='center',fontsize=20)
plt.text(1,0.9,'$x_3$',ha='center',fontsize=20)
plt.axis([-4.5,4.5,-0.1,1.1])

plt.subplot(122)
plt.grid(True,which='both')
plt.axhline(y=0,color='k')
plt.axvline(x=0,color='k')
plt.plot(XK[:,0][yk==0],XK[:,1][yk==0],'bs')
plt.plot(XK[:,0][yk==1],XK[:,1][yk==1],'g^')
plt.xlabel(r"$x_2$",fontsize=20)
plt.ylabel(r"$x_3$",fontsize=20,rotation=0)
plt.annotate(r'$\phi\left(\mathbf{x}\right)$',
            xy = (XK[3,0],XK[3,1]),
            xytext = (0.65,0.5),
            ha = 'center',
            arrowprops=dict(facecolor='black',shrink=0.1),
            fontsize=18,
            )
plt.plot([-0.1,1.1],[0.57,-0.1],'r--',linewidth=3)
plt.axis([-0.1,1.1,-0.1,1.1])

plt.subplots_adjust(right=1)

plt.show()
```

![image-20221004230333353](../typora_image/image-20221004230333353.png)

> 当前是将所指的一维x点，通过高斯函数，映射到右图的样子。

> 若修改$γ=0.1$：

![image-20221004230505332](../typora_image/image-20221004230505332.png)

> 左图中，x2,x3曲线范围变大了，考虑的更全面，过拟合风险就更低了。
>
> * **减少gamma γ** 使高斯曲线变宽，因此实例具有更大的影响范围，并且决策边界更加平滑。
>
> * **增加gamma γ** 使高斯曲线变窄，因此每个实例的影响范围都较小；决策边界最终变得更不规则，在个别实例周围摆动。

* 理想情况下会得到几维特征呢？

> 可以对每一个实例（样本数据点）创建一个地标（一维数据），此时会将$m*n$的训练集转换成$m*m$的训练集。

* SVM中利用了核函数的计算技巧，大大降低了计算复杂度

```python
# 对比实验：不同γ和C
from sklearn.svm import SVC

gamma1,gamma2 = 0.1,5
C1,C2 = 0.001,1000
hyperparams = (gamma1,C1),(gamma1,C2),(gamma2,C1),(gamma2,C2) # 四组参数，元组类型

svm_clfs = []
# 遍历不同的γ值、松弛因子
for gamma, C in hyperparams:
    # 管道：标准化 --> 支持向量机
    rbf_kernel_svm_clf = Pipeline([
    ('scaler',StandardScaler()),
    ('svm_clf',SVC(kernel='rbf',gamma=gamma,C=C))
    ])
    rbf_kernel_svm_clf.fit(X,y)
    svm_clfs.append(rbf_kernel_svm_clf)

plt.figure(figsize=(11,7))

for i,svm_clf in enumerate(svm_clfs):
    plt.subplot(221 + i)
    plot_predictions(svm_clf,[-1.5,2.5,-1,1.5])
    plot_dataset(X,y,[-1.5,2.5,-1,1.5])
    gamma,C = hyperparams[i]
    plt.title(r'$\gamma = {},C = {}$'.format(gamma,C),fontsize=16)
plt.show()
```

<img src="../typora_image/image-20221005203555022.png" alt="image-20221005203555022" style="zoom:80%;" />

# 深度学习

* 机器学习流程：数据获取、**特征工程**（难度）、建立模型、评估与应用

* 特征工程的作用

> **数据特征决定了模型的上限**，预处理和特征提取是最核心的，算法与参数选择决定了如何逼近这个上限。
>
> 深度学习就是解决特征提取这个问题。

* 机器学习常规套路

  > 1. 收集数据并给定标签
  > 2. 训练一个分类器
  > 3. 测试，评估

* k近邻算法

> 流程：
>
> 1. 计算已知类别数据集的点与当前点的距离
> 2. 按照距离依次排序
> 3. 选取与当前点距离最小的k个点
> 4. 确定前k个点所在类别的出现概率
> 5. 返回前k个点出现频率最高的类别作为当前点预测分类

* 数据库样例：CIFAR-10

> 10类标签，5万个训练数据，1万个测试数据，大小均32*32。

* 距离选择

> 图像距离计算方式：
>
> ![image-20221008185025879](E:/my_work/ML/typora_image/image-20221008185025879.png)
>
> 测试集与训练集对应像素点相减取，再取绝对值后求和，得到距离。

> 经过测试后，测试结果：部分结果还可以，但有些图没有分类成功。
>
> 其原因在于，**k近邻算法**把全局当做判断对象，而没有考虑**主体**（主要成分）在哪。
>
> 如此，**神经网络**最重要的部分就是区分主体。

## 计算方法

* 线性函数

> 从输入 --> 输出的映射：
>
> ![image-20221008190256583](E:/my_work/ML/typora_image/image-20221008190256583.png)
>
> 其中，$x$对应像素点的值，$W$对应权重参数。
>
> 那么，它们分别构成$3072 * 1$和$1 * 3072$ 的矩阵表示，则两矩阵相乘的结果为一个值；
>
> 假如是10分类任务，那么要计算出各自的分类得分，则$Wx$的值就是$10 * 3072$的矩阵；
>
> 其中，每列表示每个像素点所对应每个分类的权重参数，行代表类别的结果，$b$ 为偏置项微调结果。

> 若将图像划分成四个区域，做三分类任务：
>
> ![image-20221008192254473](E:/my_work/ML/typora_image/image-20221008192254473.png)
>
> $W$为 $3*4$ 的矩阵表示**权重参数**，$x_i$ 为 $ 4*1 $ 表示四个区域的像素点值；
>
> 通过 $W x+b$ 得到三个类别的总得分。
>
> apparently，权重参数越大，表示对这一类别重要；正负亦如此。
>
> 最终的分类结果得分，是与权重参数有关，而与数据是无关的。
>
> 考虑权重参数得到的分类结果要用**损失函数**来衡量。

## 损失函数

* 假设损失函数为：

$$
L_i = \sum_{j≠y_i}max(0,s_j-s_{y_i}+1)
$$

> 给出三个类别数据：
>
> <img src="E:/my_work/ML/typora_image/image-20221008195117772.png" alt="image-20221008195117772" style="zoom:50%;" />
>
> 其中，$s_j$ 表示预测值，$s_{y_i}$表示真实值，$+1$相当于包容程度；
>
> 如第一列数据，将该数据错误预测成了car，则代入公式有：
> $$
> max(0,5.1-3.2+1)+max(0,-1.7-3.2+1)\\[2ex]
>  =2.9+0\\[2ex]
>  =2.9
> $$
> 表示当前损失为2.9。
>
> 亦如，第二列数据得到损失为0，则表示分类较好。

* 如果损失函数的值相同，意味着两个模型一样吗？

> 不一样。

* 损失函数 = 数据损失 + 正则化惩罚项

$$
L = \frac{1}{N}\sum_{i=1}^{N}\sum_{j≠y_i}max(0,f(x_i;W)_j - f(x_i;W)y_i+1) + λR(W)
$$

> 其中，$λ$ 为正则化惩罚项，因为**神经网络**太强了，需要避免过拟合；
>
> λ越大，则惩罚力度越大；反之越小。



## Softmax分类器

* 如何把一个得分转换成一个概率值？

$$
g(z) = \frac{1}{1+e^{-z}} \\[2ex]
$$

> 先将得分映射到sigmod函数中。

* 归一化

$$
P(Y=k|X=x_i)=\frac{e^s}{\sum_je^sj} \,\,\,\,S = f(x_i;W)
$$

* 计算损失值

$$
L_i = -logP(Y=y_i|X=x_i)
$$

> 再归一化可得概率，然后再通过**log**函数的性质，求出损失值。

## 前向传播和反向传播

* 前向传播

> 上述操作步骤叫做前向传播，即输入x与w ——> 输出损失

![image-20221011210719635](../typora_image/image-20221011210719635.png)

* 反向传播
  * 利用梯度下降，多次计算后，找到最小的损失值

![image-20221011210951533](../typora_image/image-20221011210951533.png)

> 通过得到的损失值，对其进行链式求导，逐层进行计算。
>
> 由A到传播B，即由$ ∂L/∂A $得到$ ∂L/∂B $，由导数链式法则$ ∂L/∂B=(∂L/∂A)⋅(∂A/∂B)$ 实现。所以神经网络的BP就是通过链式法则求出 L 对所有参数梯度的过程。

* 复杂的例子

$$
f(w,x) = \frac{1}{1+e^{-(w_0x0+w_1x_1+w_2)}}
$$

![image-20221013182319234](../typora_image/image-20221013182319234.png)

> * 加法门单元：如$x+y=q$求导之后，均等分配
> * MAX门单元：只把梯度给最大的
> * 乘法门单元：如$x*y=q$求导之后，互换的感觉

## 神经网络整体架构

![image-20221013183703597](../typora_image/image-20221013183703597.png)

* 层次结构：

  > 输入层 —— 隐藏层 —— 输出层
  >
  > 神经网络具有层次结构，每一层都是对前一层所做出的结果再进行处理。

* 神经元

> 在输入层相当于输入的特征个数。

* 全连接

> 每个点与下一层的点都有连接；从输入层 $[1*3]$ 到隐藏层1 $[1*4]$ 是做了一个矩阵乘法 $[1*3] * [3*4] = [1*4]$
>
> （$[3*4]$是构造出的数据矩阵大小）
>
> 取得隐藏层1的结果，输入到隐藏层2中 $[1*4] * [4*4] = [1*4]$；
>
> 在隐藏层里通过反向传播计算出最好的组合方式。

* 非线性

> 通过非线性函数，在每一次构造完数据的矩阵后进行变换。

$$
基本结构：f = W_2max(0,W_1x) \\[2ex]
继续堆叠下一层：f = W_3(0,W_2max(0,W_1x))
$$

## 神经元个数对结果的影响

* 1个神经元

<img src="../typora_image/image-20221014205926609.png" alt="image-20221014205926609" style="zoom:67%;" />

* 2个神经元

<img src="../typora_image/image-20221014205952092.png" alt="image-20221014205952092" style="zoom: 67%;" />

* 3个神经元

<img src="../typora_image/image-20221014210015614.png" alt="image-20221014210015614" style="zoom:67%;" />

* 结论：神经元个数越多，决策效果越好，过拟合风险也越大

* 对于随机数据，神经网络也可以正确的找到规律：

<img src="../typora_image/image-20221014210448133.png" alt="image-20221014210448133" style="zoom:67%;" />

## 正则化与激活函数

* 正则化的作用

> 惩罚力度对结果的影响：
>
> ![image-20221014211214717](../typora_image/image-20221014211214717.png)
>
> 惩罚越小，过拟合风险越大。

* 激活函数

> 神经网络中的每个神经元节点接收上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐藏层或输出层）；
>
> 在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系， 这个函数称为**激活函数**。
>
> 激活函数其实是指如何把“激活的神经元的特征”通过函数把特征保留并映射出来，即负责将神经元的输入映射到输出端。
>
> 常用的激活函数：Sigmoid,Relu,Tanh等。

* Sigmoid

  ![image-20221016112739718](../typora_image/image-20221016112739718.png)

> sigmod的软饱和性：
>
> 其两侧导数逐渐趋近于0，具体来说，由于后向传递过程中，sigmoid向下传导的梯度包含了一个 $f'(x)f'(x)$ 因子，因此一旦输入落入饱和区， $f'(x)f'(x)$ 就会变得接近于0，导致了向底层传递的梯度也变得非常小。
>
> 此时，出现的梯度消失现象，对后面的梯度就不进行更新了。

* Relu

![image-20221016113659488](../typora_image/image-20221016113659488.png)

> 优势：
>
> * 没有饱和区，不存在梯度消失问题，防止梯度弥散；
> * 稀疏性；
> * 没有复杂的指数运算，计算简单、效率提高；
> * 实际收敛速度较快，比Sigmoid/tanh快得多；
> * 比Sigmoid更符合生物学神经激活机制。

* 数据预处理

> 不同的预处理结果会使得模型的效果发生很大的差异。
>
> <img src="../typora_image/image-20221016114603194.png" alt="image-20221016114603194" style="zoom: 67%;" />
>
> zero-centered data：去中心化。
>
> normalizedd data：标准化。

* 权重参数初始化

> 参数初始化同样非常重要。
>
> ```python
> W = 0.01 * np.random.randn(D,H)
> ```
>
> D，H代表矩阵维度，乘上0.01是为了让初始化的值尽可能的小，且趋于稳定的状态；
>
> 通常都使用随机策略来进行初始化。

## 神经网络过拟合解决方法

* DROP-OUT（七伤拳）

<img src="../typora_image/image-20221016115357498.png" alt="image-20221016115357498" style="zoom:80%;" />

> 神经网络中，节点与节点之间都是全连接的一个状态（左图）；
>
> 训练需要很多次，若在每层每一次都随机选择部分的节点进行参与，使模型更加的简单，即**DROP-OUT**；
>
> 而在测试阶段，还是使用全连接的模型进行测试即可。

# 关联规则

* 若两个或多个变量的取值之间存在某种规律性，就称为关联，如尿布和啤酒。
* 关联规则是寻找同一个事件中出现的不同项的相关性，比如在一次购买活动中所买不同商品的相关性。

* 数据集描述

| 编号  | 牛奶 | 果冻 | 啤酒 | 面包 | 花生酱 |
| ----- | ---- | ---- | ---- | ---- | ------ |
| $T_1$ | 1    | 1    | 0    | 0    | 1      |
| $T_2$ | 0    | 1    | 0    | 1    | 0      |
| $T_3$ | 0    | 1    | 1    | 0    | 0      |
| $T_4$ | 1    | 1    | 0    | 1    | 0      |
| $T_5$ | 1    | 0    | 1    | 0    | 0      |
| $T_6$ | 0    | 1    | 1    | 0    | 0      |
| $T_7$ | 1    | 0    | 1    | 0    | 0      |
| $T_8$ | 1    | 1    | 1    | 0    | 1      |
| $T_9$ | 1    | 1    | 1    | 0    | 0      |



> * 一个样本称为一个“**事务**”
> * 每个事务由多个属性来确定，这里的属性称为“**项**”
> * 多个项组成的集合称为“**项集**”

> 由k个项构成的集合
>
> * {牛奶}、{啤酒}都是1-项集
> * {牛奶、果冻}是2-项集
> * {啤酒、面包、牛奶}是3-项集

> X==>Y含义：
>
> * X和Y是**项集**
> * X称为**规则前项**（antecedent）
> * Y称为**规则后项**（consequent）

* 事务仅包含涉及到的项目，而不包含项目的具体信息

> 在超市中的关联规则挖掘问题中，事务是顾客一次所购买的商品，并不包含这些商品的具体信息，如数量、价格等。

## 支持度（support）

* 定义：一个项集或者规则在所有事务中出现的频率，$\sigma(X)$：表示项集X的支持度计数

* 项集XD支持度：$s(X) = \sigma(X)/N$ 

* 规则 X==>Y 表示物品及X对物品集Y的支持度，也就是物品集X和物品集Y同时出现的频率。

  > 如：100个顾客到商场购买物品，其中30个顾客同时购买了啤酒和尿布，那么上述的关联规则的支持度就是30%。

## 置信度（confidence）

* 定义：确定Y在包含X的事务中出现的频率程度。$c(X → Y) = \sigma(X∪Y)/\sigma(X)$

* $p(Y|X) = p(XY)/p(X)$

* 置信度反应了关联规则的可信度——购买了项目集X中的商品的顾客同时也购买了Y中商品的可能性有多大

  > 如：购买薯片的顾客中有50%的人购买了可乐，则置信度为50%。

* 例：

| 交易ID | 购买的商品 |
| ------ | ---------- |
| 1      | A,B,C      |
| 2      | A,C        |
| 3      | A,D        |
| 4      | B,E,F      |

* 若 (X,Y) ==> Z：

> 支持度：交易中包含（X、Y、Z）的可能性
>
> 置信度：包含（X，Y）的交易中也包含Z的条件概率

* 设最小支持度为50%，最小可信度为50%，则可得到：

  * A==>C(50%,66,6%)

    > A和C同时出现的概率为50%，出现了A的前提下有C的概率为2/3即66.6%。

  * C==>A(50%,100%)

* 如何确定相关性？

> 若关联规则 X->Y 的支持度和置信度，分别大于或等于用户指定的**最小支持率minsupport**和**最小置信度minconfidence**，则称关联规则 X->Y 为**强关联规则**，否则称关联规则 X->Y 为**弱关联规则**。

# TensorFlow

## 源码

## 简单机器学习实例

* 线性方程组方程组

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
```

```python
# x和y是特征
number_of_datapoints = 100
x = np.random.uniform(low = -5,high = 5,size = (number_of_datapoints,1)) # 范围是(low,high),number行，1列的数据
y = np.random.uniform(-5,5,size=(number_of_datapoints,1))
# 打印前五个
x[:5,:].round(2) # 四舍五入，保留两位小数

# 加入噪声稳定预测模型
noise = np.random.uniform(low = -1,high=1,size=(number_of_datapoints,1))
# 创建方程
z = 7*x+6*y+5+noise

# 将两个矩阵按列合并
input = np.column_stack((x,y)) 
input[:5]
```

* 定义神经网络

> Keras库提供了一个Sequential API用于定义网络模型。

```python
model = tf.keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])]) # units=1：单层网络，单个神经元输出单个值
```

* 编译模型

> 定义学习过程：目标损失函数，优化器，评价指标。

```python
model.compile(optimizer='sgd', # 随机梯度下降优化器
              loss='mean_squared_error', # 均方误差为损失函数和评价指标
              metrics=['mse']
             )
```

* 训练网络

> 需要经过多次迭代(epoch)来完成模型的训练过程； 
>
> 在每个epoch后，要保存并监控损失函数值，确保其在正确方向进行优化网络。

```python
from tensorflow.keras.callbacks import History
# 创建历史对象用来保存
histroy = History()

# 模型训练
'''
input:数据输入，z:目标值，epochs:迭代次数，verbose:是否观察训练进度
validation_split:取20%数据用于验证集，callbacks:中间监控数据的存储位置
'''
model.fit(input,z,epochs=15,verbose = 1, 
          validation_split=0.2,callbacks=[histroy])
```

* 检查结果

```python
# 打印每个epoch，保存了每个损失值和评价指标x
print(histroy.history.keys())
# dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])
```

```python
# 画图：损失值
plt.plot(histroy.history['loss'])
plt.plot(histroy.history['val_loss'])
plt.title('Accuracy')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','validation'],loc='upper right')
plt.show()
```

<img src="../typora_image/image-20221204164922016.png" alt="image-20221204164922016" style="zoom:67%;" />

> 如图，在第3个epoch后损失值完成最小化。

```python
# 画图：预测值与真实值
plt.plot(np.squeeze(model.predict_on_batch(input)),np.squeeze(z))
plt.xlabel('predicted output')
plt.ylabel('real output')
plt.show()
```

<img src="../typora_image/image-20221204164951567.png" alt="image-20221204164951567" style="zoom:67%;" />

* 预测

```python
print("Predicted z for x=2, y=3 ---> ", model.predict([[2,3]]).round(2))
# Predicted z for x=2, y=3 --->  [[34.89]]
```

## 二分类

# TDA

## 持续同调

* [拓扑数据分析－持续同调（一） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/31734839)

* 定义拓扑空间

  * **拓扑空间：** 是一组有序对 (X,τ) ，其中 X 是集合， τ 是 X 的子集的集群，他们满足以下性质：

    > 1. 空集和 X 属于 τ ，
    > 2. τ 中任意多个元素的并仍属于 τ ，
    > 3. τ 中有限个元素的交仍属于 τ 。

  * τ 的元素称为 **开集**，集群 τ 称为 X 上的一个拓扑。

  * 如：$X=\{a,b,c\},τ=\{∅,{a},{a,b,c}\}$ ， X 上有一个有效拓扑，即 τ 。

* 可视化有限拓扑

  * **特殊化预序**： x≤y ，当且仅当所有包含 x 的开集都包含 y 。

    > 如：$τ=\{∅,\{a\},\{b\},\{a,b\},\{b,c\},\{a,b,c\}\}$，判断c≤b，即包含c的开集都包含b；
    >
    > 则有c的开集：$\{b,c\},\{a,b,c\}$ ，发现包含c的开集（τ的元素）都包含b，则c≤b为真。
    >
    > 
    >
    > 预序关系的有向图，如果x≤y，则x指向y；根据上例，则有：

![img](https://pic4.zhimg.com/80/v2-81a2e467c87e29e384dd8e7494aff15b_720w.webp)

* 特殊化预序例子：$Z=\{a,b,c,d\}$，$T_Z=\{Z,∅,\{b\},\{a,b\},\{b,c,d\}\}$  ，则图为：

![img](https://pic4.zhimg.com/80/v2-3fac719f0fafa315990b207a5c7d451b_720w.webp)

* 连通性

  * 如果 X 不能表示为两个非空互斥的开集的并，我们说拓扑空间 (X,τ) 是连通的。也就是说，如果 τ 中两个非空互斥的子集的并是 X ，此拓扑空间是不连通的。

    > 如，$X=\{a,b,c\}$，$τ=\{∅,\{a\},\{b\},\{a,b\},\{b,c\},\{a,b,c\}\}$ ，因为两个互斥开集的并集$\{a\}∪\{b,c\}=X$，这个拓扑空间不连通。
    >
    > 而集合Z的图说明它的空间是连通的。（类似于极大连通图）

* 度量空间

  * 是一对有序对 $(M,d)$ ，其中 $M $是集合，而$ d $是 $M $的度量标准，也就是说函数 ：$d：M×M→R $（这定义了函数 $d$ 将 $M $中元素的每一个有序对映射到实数集 R 的一个元素上），对于任何 $x,y,z $属于 $M$ ，有：

    > 1. d(x,y)≥0 （所有距离都是非负的）
    > 2. d(x,y)=0 当且仅当 x=y
    > 3. d(x,y)=d(y,x) （距离是对称的）
    > 4. d(x,z)≤d(x,y)+d(y,z) （从 x 到 z 的直线距离必不大于经过任何中间点）

  * **度量空间**从集合中获取两个元素并返回这两个元素的度量距离。
  * 只要你能定义一个函数，它能计算出集合中任意两个元素之间的距离，它就是一个有效的度量空间。

* 连续性

  * **同态**： 如果两个拓扑空间之间存在函数$ f:X→Y $，其中 X 和 Y 是两个拓扑空间 $(X,τ_X) $和 $(Y,τ_Y)$ ，那么它们是同态的，其中 f 满足以下条件：

    > 1. f 映射了 X 到 Y 一一对应的关系（双射）。
    > 2. f 是连续的。
    > 3. 反函数 f−1 是连续的。

  * **连续函数**： 对于两个拓扑空间$ (X,τ_X) $和$ (Y,τ_Y) $而言，如果每个元素 $V∈τ_Y $（正如 $Y $的开集）的原像$ f^{−1}(V) $存在于$ X $中，那么函数 $f $是连续的。
  * 当且仅当函数 ：$f：X→Y $是保序的： $X $中 $x≤y $表示$ Y $中$ f(x)≤f(y) $，函数$ f $是**连续**的。

  * 如：让 $X=\{a,b,c\} $，它的拓扑$ τ_X=\{∅,\{a\},X\} ； Y=\{d\} $，它的拓扑 $τ_Y=\{∅,Y\} $。连续函数 ：$f：X→Y $如下图描述：

    ![img](https://pic1.zhimg.com/80/v2-8859143d12e27633532f9550da12dfe4_720w.webp)

  * 原像$ f^{−1}({d})={a,b,c} $是 $X $的开集，因此函数是连续的。

* 单纯性和单纯复形

  * 单纯形是一个三角形到任意维度的泛化。

  ![img](https://pic4.zhimg.com/80/v2-66c23cb711faeb591be3e22c49495c27_720w.webp)

  * 纯复形是由不同的单纯形“粘合”起来的。
  * 例子（单纯复形）

  ![img](https://pic2.zhimg.com/80/v2-5e57c7e4c03f1289ac6473ca82571621_720w.webp)

  

  * 左边两个三角形共有一条边连接起来，并用一维单纯性（线段）连接右三角形，该图形称之为2维单纯复形，即组成该复形的最高单纯形是2维单纯形。

    > 如1为单纯形（线段）的表面是个点（0维单纯形），2维单纯形（三角形）的表面是线段，3维单纯形（正四面体）的表面是三角形（2维单纯形）。

  * **抽象单纯形**： 一个抽象的单纯形是任何一个有限的顶点集合。例如，单纯形 $J=\{a,b\} $和单纯形 $K=\{a,b,c\} $分别代表了1维单纯形和2维单纯形。

  * **单纯复形**： 一个单纯复形 $K $是单纯形的集合，它满足以下条件：

    > 1. K 中任意一个单纯形的任意面仍属于 K
    > 2. K 中任意两个单纯形 σ1,σ2 的交集是空集或者二者共享的面中的其中一个。

  * 如该单纯复形用数学定义：

  ![img](https://pic3.zhimg.com/80/v2-ed550b8baa53fdc9a7c36070d7666a86_720w.webp)

  * 该复形用集合表示，先列出0维单纯形，再1维、2维；该复形满足第二个条件，因为所有高维单纯形的任何面都会先被列出，直到列出所有顶点。
    $$
    K=\{\{a\},\{b\},\{c\},\{d\},\{f\},\{a,b\},\{a,c\},\{b,c\},\{c,d\},\{d,f\},\{d,e\},\{e,f\},\\[2ex]\{a,b,c\},\{d,e,f\}\}
    $$
    
  * 常见单纯复形
    * 切赫复形（Čech complex），维托里斯-里普斯复形（Vietoris–Rips complex），阿尔法复形（alpha complex），威特尼斯复形（witness complex）。
  
* 构建VR复形

  * 通过在P中最初的连结点的边缘低于一些互相ϵ任意定义的距离在点云 $P⊆Rd $（ d -维空间的子集 P ）中构建了 VR 复形。
  *  $R^2 $构建从圆形结构中取样的VR复形的可视化的主要步骤(从左到右)：

  ![img](https://pic1.zhimg.com/80/v2-53e5c201b0520c90cae83719e2bc2600_720w.webp)![img](https://pic4.zhimg.com/80/v2-6f13b30d39e6ec43e5cdac4b1289cbb3_720w.webp)![img](https://pic3.zhimg.com/80/v2-347d33cf0199f71fce7971fc2d32799a_720w.webp)![img](https://pic4.zhimg.com/80/v2-db0d450b97906e59055d59af2dec8643_720w.webp)

  * 以 ϵ -球（ ϵ 为半径的圆），围绕P上的每一个点，将该点与其圈内的其他点连接起来。
  * 一般来说，一个$d$ 维空间点的球是$(d-1)$ 维空间中围绕那个点的球的泛化。
  * 所以 $\R $中的球是一条围绕着某个点的线段， $\R^2 $中的球是圆，$ \R^3 $中的球是球体，以此类推。

  > 如何选择ϵ？
  >
  > 你只需要选择多个不一样的ϵ然后看看在一个有意义的VR复形中它的结果。

  * **VR 复形**：如果在d-维空间中有一个点集P， $\R^d $的子集，那么比例 ϵ 的VR复形 $V_ϵ(P) $是这样定义的： $V_ϵ(P)={σ⊆P∣d(u,v)≤ϵ,∀u≠v∈σ}$
  * 意思是：比例 ϵ 下的VR复形是集合$ V_ϵ(P) $中的 P 的所有子集 σ ，也就是说 σ 中任何不同点之间的距离不大于参数 ϵ 。

## 

