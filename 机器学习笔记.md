# 机器学习

## 什么是机器学习

* 计算机程序从经验**E**中学习，解决某一任务**T**，进行某一性能度量**P**，通过**P**测定在T上的表现因经验**E**而提高。

> T：是否对垃圾邮件进行标记
>
> E：是否把邮件标记为垃圾邮件
>
> P：正确归类邮件的比例

## 监督与无监督

* 监督学习（Supervised Learning）：处理带有特征的数据，即数据都有正确的答案。

  > 回归（Regression）：设法预测连续值的属性输出 
  >
  > 分类（Classification）：设法预测一个离散值输出（0 或 1）

* 无监督学习（Unsupervised learning）：数据都具有相同的标签或都没有标签，事先不知道数据是什么类型，从中寻找其特征。

  > 聚类（Clustering）：分簇，即对数据进行归类

<hr>

# 单变量线性回归

## 模型描述

* 训练集

  > **m**：表示训练样本数量
  >
  > **x**：输入特征
  >
  > **y**：输出变量，即预测值
  >
  > (x,y)：一个训练样本
  >
  > (x<sup>i</sup>,y<sup>i</sup>)：第 i 个训练样本

  > 用训练集作模型训练，测试集去获得预测值。

* 线性回归

  > h：假设函数
  >
  > *θ<sub>i</sub>*：需要预测的系数

  $$
  h_θ(x) = θ_0+θ_1x_1+θ_2x_2\\[2ex]
  $$

  > $θ_1$等称为权重项，$θ_0$称为偏置项；
  >
  > 偏置项的影响对结果不大，在拟合曲线时作为微调；权重项的影响较大。
  >
  > 将函数转化为**矩阵**形式：$h_θ(x)=\sum_{i=1}^mθ_ix_i=θ^Tx\\[2ex]$
  >
  > 由于函数省略了$x_0$，一般在处理数据之前，要加一个一维向量：$x_0 = [1 1 1 ... 1]^T$
  >
  > ```python
  > # 给原始数据加入一列偏置项
  > X_b = np.c_[np.ones((100,1)),X] # np.c_[]:矩阵拼接
  > ```

<hr>

## 误差

> 真实值和预测值之间肯定是要存在差异的。
>
> $y^{(i)} = θ^Tx^{(i)}+ϵ^{(i)}$ ：$ϵ$ 来表示误差，y 表示真实值，$θ^Tx^{(i)}$ 表示预测值。
>
> **期望误差越趋于0，则预测越好。**

> 误差$ϵ^{(i)}$ 是**独立**并且具有相同的分布，并且服从**均值**为0**方差**为 $θ^2$ 的高斯分布。
>
> <img src="https://pic3.zhimg.com/v2-933150699d3c7538e53ead73adc25782_1440w.jpg?source=172ae18b" alt="高斯分布" style="zoom:67%;" />

* 均方误差

```python
from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_predict)
```



<hr>

## 代价函数

> $$
> \frac{1}{2m}\sum_{i=1}^m(h_θ(x^i) - y^i)^2
> $$
>
> h(x)-y是指尽量使 h(x) 和 y 的差异最小，第i号对应的预测结果，减去第i号对应的实际值，所得差的平方再相加
>
> m指样本数量，尽量减少我们的平均误差，即尽量减少其 1/2m
>
> * 代价函数
>
> $$
> J(θ_0,θ_1) =\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i) - y^i)^2\\[2ex]
> minJ(θ_0,θ_1)
> $$
>
> 上述即求**θ<sub>0</sub>**和**θ<sub>1</sub>**的最小值，代价函数也称为平方误差函数、平方误差代价函数

* 推导：假设函数
  $$
  将假设函数简化为一个参数，方便推导：h_θ(x) = θ_1x\\[2ex]
  代价函数：J(θ_1)=\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i) - y^i)^2\\[2ex]
  minJ(θ_1)
  $$

  > 可以看做使**θ<sub>0</sub>**=0

  

  > 给出**(x,y)**的值：(1,1) (2,2) (3,3)
  >
  > ![image-20220415213550601](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220415213550601.png)
  >
  > 使**θ<sub>1</sub>**=1，推导***J(θ<sub>1</sub>)=0***：
  > $$
  > J(θ_1)=\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i) - y^i)^2\\[2ex]
  > =\frac{1}{2m}\sum_{i=1}^m(θ_1x^i - y^i)^2\\[2ex]
  > =\frac{1}{2m}(0^2+0^2+0^2)=0
  > $$
  > 得出***J(θ<sub>1</sub>)=0***，即右图J的函数值为0
  >
  > 注意，J是关于θ的函数，**θ=1**，则有J(1)=0
  >
  > **同理**，使**θ<sub>1</sub>**=0.5，**θ<sub>1</sub>**=0等，J的图像则有：
  >
  > ![image-20220415214728746](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220415214728746.png)
  >
  > 由上图，每个**θ<sub>1</sub>**都对应一个不同的假设函数h，和***J(θ<sub>1</sub>)***的值；
  >
  > 最后取最小的 minimize ***J(θ<sub>1</sub>)***：**θ<sub>1</sub>**=1时，J=0，即最小

## 梯度下降

> 当我们得到一个目标函数后，如何进行求解？直接求解？
>
> 机器学习的套路就是我交给机器一堆数据，然后告诉它什么样的学习方式是对的（目标函数），然后让它朝着这个方向去做。
>
> 而这个过程是**循序渐进**，一步一步迭代完成的。

* 有一些J函数，需要一种算法来得到最小化 J

> 1. 初始化**θ<sub>0</sub>**，**θ<sub>1</sub>**，一般为0
> 2. 一点一点的改变**θ<sub>0</sub>**，**θ<sub>1</sub>**的值，直到找到J的最小值 

![image-20220417211110776](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220417211110776.png)

> 想象站在山的最顶端，想要下山，则需选择最低点，可选的路径依次往下走，如图所示；
>
> 寻找山谷的最低点，也就是我们的目标函数终点。

* 公式

$$
重复赋值直到收敛：\\[2ex]
θ_j:=θ_j-α\frac{∂}{∂θ_j}J(θ_0,θ_1)\,\,\,\,\,\, (for\, j=0\, and \, j=1)
$$

> α：学习率，用于控制梯度下降时，需要多大的值，即值越大，下降速度越快
>
> 在此公式中，**θ<sub>0</sub>**，**θ<sub>1</sub>**在同时不断更新其值，即同步更新：
> $$
> temp0:=θ_0-α\frac{∂}{∂θ_0}J(θ_0,θ_1)\\[2ex]
> temp1:=θ_1-α\frac{∂}{∂θ_1}J(θ_0,θ_1)\\[2ex]
> θ_0:=temp0 \\[2ex]
> θ_1:=temp1
> $$

* 公式推导	

  <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220417213433892.png" alt="image-20220417213433892" style="zoom:67%;" />

> 第一个坐标，偏导数为正时，**θ<sub>1</sub>**不断的向左靠近，逼近最小值
>
> 第二个坐标，偏导数为负时，**θ<sub>1</sub>**不断的向右靠近，逼近最小值

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220417213838226.png" alt="image-20220417213838226" style="zoom:67%;" />

> 如果α太小：逼近的速度会很慢
>
> 如果α太大：在逼近的过程中，有可能越过最小值
>
> 当接进局部最低时，导数值会自动越来越小，因此不需要减少α的值；
>
> 当达到最小值，即局部最优点时，斜率为零，则解始终会保持在局部最优点；
>
> 这也保证了即使α值不变，梯度下降法也能收敛到局部最低点。

## 线性回归的梯度下降

* 将梯度下降应用到最小化平方差代价函数

$$
\frac{∂}{∂θ_j}J(θ_0,θ_1) =\frac{∂}{∂θ_j}\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i) - y^i)^2\\[2ex]
=\frac{∂}{∂θ_j}\frac{1}{2m}\sum_{i=1}^m(θ_0+θ_1x^i - y^i)^2\\[2ex]
j=0:\frac{∂}{∂θ_0}J(θ_0,θ_1) = \frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i) \\[2ex]
j=1:\frac{∂}{∂θ_1}J(θ_0,θ_1) = \frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i \\[2ex]
$$

* 线性回归的梯度下降公式
  $$
  重复赋值直到收敛：\\[2ex]
  θ_0:=θ_0-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)\\[2ex]
  θ_1:=θ_1-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i\\[2ex]
  $$

> 同理，**θ<sub>0</sub>**，**θ<sub>1</sub>**的值需同步更新
>
> 该公式又称为**Batch梯度下降公式**，梯度下降的每一步都要遍历整个训练集的样本

# 多变量线性回归

## 多元线性回归

* 当数据集带有多个特征时进行预则

> n：表示特征数量
>
> x<sup>i</sup>：表示第 i 个样本的全部特征，用向量表示
>
> x<sup>i</sup><sub>j</sub>：表示第 i 个训练样本中第 j 个特征量的值
>
> 多个特征时，假设函数为：
> $$
> h_θ(x) = θ_0+θ_1x_1+θ_2x_2+……+θ_nx_n\\[2ex]
> 其中，定义x_0=1\\[2ex]
> 向量形式为：
> x=\begin{pmatrix} x_0  \\ x_1 \\ x_2\\ \vdots\\x_n \end{pmatrix} 
> θ=\begin{pmatrix} θ_0  \\ θ_1 \\ θ_2\\ \vdots \\θ_n \end{pmatrix} \\[2ex]
> 则假设函数:h_θ(x) = θ_0x_0+θ_1x_1+θ_2x_2+……+θ_nx_n\\[2ex]
> =θ^Tx
> $$
> 代价函数：
> $$
> J(θ)=\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i) - y^i)^2\\[2ex]
> $$
> 梯度下降：
> $$
> θ_j:=θ_j-α\frac{∂}{∂θ_j}J(θ)\,\,\,\,\,\, (j=0,...,n)
> $$
> 当**n>=1**时，多元梯度下降公式的推导：
> $$
> θ_j:=θ_j-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j \,\,\,\, (θ_j:j=0,...,n)\\[2ex]
> 推导：
> θ_0:=θ_0-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_0\\[2ex]
> θ_1:=θ_1-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_1\\[2ex]
> θ_2:=θ_2-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_2\\[2ex]
> ...\\[2ex]
> \frac{∂}{∂θ_j}J(θ) =\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j\\[2ex]
> =\frac{1}{2m}X^T*(X*θ-y)
> $$

## 特征缩放

* 面对多个特征时，如果能确保不同特征的取值在相近的范围内，梯度下降时的收敛会更快。

> 将特征取值约束到 **-1≤ x<sub>i</sub> ≤ 1**

* 均值归一化、标准化

> 用 **x<sub>i</sub> - µ<sub>i</sub>** 取代 x<sub>i</sub> ，让特征值具有0的平均值。（不要应用于x<sub>0</sub>=1）
> $$
> E.g. \,\,x_1 = \frac {size - 1000} {2000} \\[2ex]
> x2 = \frac{bedrooms -2}{5} \\[2ex]
> -0.5≤x_1≤0.5，-0.5≤x_1≤0.5
> $$
> 假设房子的平均面积为1000，则用当前 x1 的面积减去1000，在除以2000；
>
> 类似的，平均一套房有两间卧室，则用当前 x2 的房间数量减去2，再除以5；
>
> 当然，数据集中每套房屋的面积不超过2000，房间数量不超过5。
>
> 则可得到公式：
> $$
> \frac{x_i-µ_i}{s_i} \\[2ex]
> µ_i是数据集中特征x_i的平均值；\\[2ex]s_i是该特征值的范围，即最大值减去最小值（或取标准差）
> $$

```python
# 归一化 容易受到异常值的影响，只适合传统精确小数据场景
scaler = MinMaxScaler(feature_range=(0,1)) # 范围 0~1
minmax_data = transfer.fit_transform(data[['param1'],['param2']]
# 标准化 异常值影响小，常用
scaler = StandardScaler()   
inmax_data = scaler.fit_transform(data[['param1'],['param2']]                                     
```

## 学习率 α

> 相当于梯度下降的步长，对结果会产生巨大的影响，一般选用小一些的值。

* 梯度下降就是为你找到一个 θ值，并且让它能够帮助你最小化代价函数 **J(θ)**

> 要最小化J(θ)，则要经过迭代，且每一步迭代后J 都应该下降；
>
> 在面对不同问题时，迭代的次数也不同，有时会迭代30次，有时也会300次甚至更高；
>
> 所以，梯度下降需要多少步迭代才能收敛，是很难预判的。
>
> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220423224114570.png" alt="image-20220423224114570" style="zoom: 50%;" />
>
> 但可以通过图来看收敛程度（x轴为迭代次数）。

* 自动收敛测试

  > 当J 函数经过迭代后，小于一个很小的数 ε，则这个测试判断函数已收敛。

* 当梯度下降算法无效时，会出现J函数不断变大，或者越过最小值的情况

  > <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220423224404246.png" alt="image-20220423224404246" style="zoom:50%;" />
  >
  > <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220423224427502.png" alt="image-20220423224427502" style="zoom:50%;" />
  >
  > 出现这些情况时，需要用到较小的 **α**；
  >
  > * 总结：
  >
  >   * **只要 α 足够小，则每次迭代之后 J 都会下降；**
  >
  >   * **但如果α太小，梯度下降则会收敛得很慢；**
  >
  >   * **如果α太大，J函数也会出现收敛缓慢的现象。**
  >
  > 可以尝试选择 α 的值：
  >
  > ……，0.001，0.003    ，0.01，0.03   ，0.1， 0.3  ，1，……

  ## 特征与多项式回归

  * 多项式回归可用于解决复杂的线性或非线性函数

  > 假设有一组房屋的数据集，其中的特征：frontage表示宽度，depth表示纵向深度；
  >
  > <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220423225534670.png" alt="image-20220423225534670" style="zoom:50%;" />
  >
  >
  > $$
  > 假设函数(预测房屋价格)：h_θ(x) = θ_0+θ_1*frontage + θ_2 * depth
  > $$
  > 设新的特征 Area = frontage * depth，则：
  > $$
  > 假设函数(预测房屋价格)：h_θ(x) = θ_0+θ_1*Area
  > $$
  >
  >
  > 创造新的特征可以让我们得到一个更好的模型。

  * 如何将模型与数据进行拟合？

  > 如图，当用二次函数拟合时，会有下降趋势；三次函数拟合时较好。
  >
  > <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220423230903030.png" alt="image-20220423230903030" style="zoom: 50%;" />
  >
  > 按照之前的假设，我们知道如何拟合，则有三次函数：
  > $$
  > h_θ(x) = θ_0+θ_1x_1 + θ_2x_2+θ_3x_3 \\[2ex]
  > ⟹\,\,= θ_0+θ_1(size)+ θ_2(size)^2+θ_3(size)^3 \\[2ex]
  > x1=(size)\\[2ex]
  > x2=(size)^2\\[2ex]
  > x3=(size)^3
  > $$
  > 如果这样选择特征（size），则特征缩放就更为重要了，可以看到size的范围是：
  >
  > size :1-1000 , size<sup>2</sup> : 1-1000,000 , size<sup>3</sup> : 1- 10<sup>9</sup>
  >
  > 这样才能将值的范围变得具有可比性

  * 特征的选择

  > 前面讲到，使用二次函数拟合效果不是很好，房价会有下降的情况；
  >
  > 当在二次函数中使用**平方根**时，效果会有所改善：
  > $$
  > h_θ(x) = θ_0+θ_1(size) + θ_2(size)^2\\[2ex]
  > ⟹
  > h_θ(x) = θ_0+θ_1(size) + θ_2\sqrt{size}\\[2ex]
  > $$

## 正规方程（最小二乘法）

* 正规方程提供了一个求θ的解析解法，在不需要迭代的情况下，直接解出θ的最优值

> 假设：下列函数中，θ是一个数值；要求出θ的最小值，一般做法为对θ求导，令其为零……
>
> 若有θ<sub>0</sub>,θ<sub>1</sub>,……，θ<sub>n</sub>这么多个数，亦如此，很麻烦。
> $$
> J(θ) = aθ^2+bθ+c\,\,\,\,(θ∈R)
> $$
> 假设有**m**个训练样本  (  (x<sup>1</sup>,y<sup>1</sup>),......,(x<sup>m</sup>,y<sup>m</sup>) )  ，和 **n** 个特征变量，则有以下矩阵形式：
> $$
> x^i=\begin{bmatrix} x^i_0 \\ x^i_1 \\x^i_2 \\ \vdots \\ x^i_n  \end{bmatrix} ∈R^{n+1}
> $$
> 对其进行这样处理，可得**设计矩阵**：
> $$
> X = \begin{bmatrix}\cdots (x^1)^T \cdots\\\cdots (x^2)^T \cdots\\ \cdots (x^3)^T \cdots \\ \vdots \\ \cdots (x^m)^T \cdots \end{bmatrix} \\[2ex]
> m *(n+1)
> $$
>
>
> 其中，X 为m*(n+1)的矩阵；
> $$
> E.g. \,\,\, if \,\,\, x^i= \begin{bmatrix} 1 \\ x^i_1  \end{bmatrix} \\[2ex]
> ⟹
> X= \begin{bmatrix} 1& x^1_1 \\ 1& x^1_2 \\  \vdots& \vdots \\ 1&x^1_m  \end{bmatrix} y=\begin{bmatrix} y^1 \\ y_2 \\\vdots\\y^m \end{bmatrix}\\[2ex] 
> m*2
> $$
> 这就是如何构建矩阵X，当得出 X 和 y 后，得出最优解：**θ = ( X<sup>T </sup>X)<sup>-1</sup> X<sup>T</sup> y** （Octave: `pinv(X' *X)*x' *y`）
>
> ```python
> # 正规方程
> theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) 
> # np.linalg.inv():求逆
> ```
>
> **得出的 θ 会最小化线性回归的代价函数 J(θ)**，且使用正规方程不用特征缩放。

* 正规方程和梯度下降的优缺点

> 假设有 m 个训练样本，n 个特征变量；
>
> |        梯度下降        |                           正规方程                           |
> | :--------------------: | :----------------------------------------------------------: |
> | 需要选择学习速率 **α** |                       不需要选择 **α**                       |
> |    需要更多次的迭代    |                          不需要迭代                          |
> |    n 较大时仍然适用    | 需要计算 ( X<sup>T </sup>X)<sup>-1</sup> ，并且当n较大时，求解速度很慢 |
>
> 当 n >10000 时，会选择用梯度下降或者其他算法。
>
> 机器学习中，核心的思想还是迭代更新。

## 正规方程在矩阵不可逆时的解决方法

* **θ = ( X<sup>T </sup>X)<sup>-1</sup> X<sup>T</sup> y** 中，矩阵 X<sup>T </sup>X 不可逆怎么办？

> 这种情况很少发生，在 **Octave** 中，存在两个求逆矩阵的函数：`pinv` ，`inv`；
>
> 其中，即使在矩阵不可逆情况下，`pinv`也能求解；
>
> 第一种不可逆的原因：出现多余的特征，则删除其中一个
> $$
> E.g. \,\,\, x_1 = size \,\,\,in \,\,\, feet^2 \\[2ex]
> x_2=size\,\,\, in \,\,\, m^2 \\[2ex]
> 其中1m=3.28feet，则 x_1 = (3.28)^2 x_2
> $$
> 第二种不可逆的原因：过多的特征（m<n），则需去除一些特征，或正则化；

# 逻辑回归


* 也称对数几率回归

## 分类

* 邮件分类（垃圾邮件？）、网络交易（是否欺诈）、肿瘤分类（良性或恶性）都属于分类问题

> 在这些问题中，我们尝试预测的变量y，都有两个可取值 **{0,1}**（离散值），即垃圾邮件或非垃圾邮件、欺诈或非欺诈、恶性或良性；
>
> **0：表示负类，通常指没有某样东西**
>
> **1：表示正类**

* 如何开发一个分类算法？

> 当肿瘤分类问题用线性回归算法解决时：
>
> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220430213757490.png" alt="image-20220430213757490" style="zoom: 67%;" />
>
> 我们假设一个阈值为0.5，当h ≥ 0.5时，y=1；当h < 0.5 时，y=0；
>
> 从上图中看到，前8个 ×（样本） 中，小于0.5的有四个，大于等于0.5的有四个，则用线性回归很好预测了肿瘤恶性还是良性；
>
> 但当我们增加一个样本时，阈值0.5就往右移动了，造成了错误预测。

> * 分类的结果： y=0 或 1
>
> * 线性回归h的结果： h(x)>1 或 <0 
>
> 上述结果表明，线性回归不能很好的解决分类问题，所以有分类算法：
>
> **逻辑回归： 0 ≤ h<sub>θ</sub>(x) ≤ 1**，其特点是预测值都处于 0和1 之间，且不会大于1或小于0。
>
> （逻辑回归不是回归算法，是分类算法）

## 假设陈述

* 逻辑回归模型

> $$
> h_θ(x) = g(θ^Tx) \\[2ex]
> g(z) = \frac {1} {1+e^{-z}} \\[2ex]
> ⟹ h_θ(x) = \frac {1} {1+e^{-θ^Tx}}\\[2ex]
> 其中，θ_0+θ_1x_1+...+θ_nx_n =\sum_{i=1}^mθ_ix_i = θ^Tx
> $$
>
> 其中，g称为 **Sigmoid function**，也称为**逻辑函数（Logistic function）**
>
> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220430220300112.png" alt="image-20220430220300112" style="zoom: 80%;" />
>
> 在此函数图像中，z趋于负无穷时，g趋于0；z趋于正无穷时，g趋于1；
>
> **由于g在（0,1）之间，所以h也一定在0到1之间**。

* h<sub>θ</sub>(x) 的输出

> h<sub>θ</sub>(x)  = 假设输入 x 得到 y=1 的概率估计
> $$
> Example: if \,\, x= \begin{bmatrix} x_0 \\ x_1\end{bmatrix} = \begin{bmatrix} 1 \\ tumorSize \end{bmatrix}\\[2ex]
> h_θ(x) = 0.7 \\[2ex]
> $$
> 上式为肿瘤预测问题，给定x后给出一个预测值为0.7，则该病人有70%几率为恶性；
>
> 化为概率公式有：h<sub>θ</sub>(x) = p(y=1|x ; θ)，即给定x（病人特征）的条件下，y=1的概率，概率参数为θ。
>
> 由于y的值必需是0或1，则：
> $$
> P(y=0|x;θ) + P(y=1|x;θ) = 1  \,\, 
> \\[2ex]
> P(y=0|x;θ) = 1-P(y=1|x;θ)\\[2ex]
> $$
> 其中，两种结果概率相加为1，第二式可为：**P(y=0|x;θ) = 1-h<sub>θ</sub>(x)**
>
> 整合：$P(y|x;θ) = (h_θ(x))^y(1-h_θ(x))^{1-y}$

## 决策界限

* 假设函数何时会将y预测为1，何时为0

$$
h_θ(x) = g(θ^Tx) \\[2ex]
g(z) = \frac {1} {1+e^{-z}} \\[2ex]
$$

> 假设：当 h<sub>θ</sub>(x) ≥ 0.5 时，y=1; h<sub>θ</sub>(x) < 0.5 时，y=0。
>
> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220430220300112.png" alt="image-20220430220300112" style="zoom: 80%;" />
>
> 根据 g 函数的图像，当 z ≥ 0时，g ≥ 0.5；
>
> 由 h<sub>θ</sub>(x) = g(θ<sup>T</sup>x) ：
>
> * 当 **θ<sup>T</sup>x ≥ 0** 时，h<sub>θ</sub>(x) ≥ 0.5 ，则 **y = 1**；
>
> * 当**θ<sup>T</sup>x ＜ 0** 时，h<sub>θ</sub>(x) < 0.5 ，则 **y = 0** 。

> 假设有一训练集，其假设函数为：
> $$
> h_θ(x) = g(θ_0 + θ_1x_1+θ_2x_2) \\[2ex]
> 设θ = \begin{bmatrix} -3 \\ 1 \\ 1\end{bmatrix}
> $$
> 将 θ 带入假设函数有： 若 -3 + x<sub>1</sub> + x<sub>2</sub> ≥ 0 ，则 y=1 ；或 x<sub>1</sub> + x<sub>2</sub> ≥ 3 同理；
>
> 则 x<sub>1</sub> + x<sub>2</sub> = 3 的图像在训练集中为：
>
> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220502220221839.png" alt="image-20220502220221839" style="zoom: 80%;" />
>
> 当大于直线  x<sub>1</sub> + x<sub>2</sub> = 3（右上方） 的区域预测为 y=1 ；相反，为 y=0 的区域。
>
> 我们称 x<sub>1</sub> + x<sub>2</sub> = 3 这条直线为 **决策边界**，即这条线处于 **h<sub>θ</sub>(x) = 0.5** 上，将整个平面分成了两部分；
>
> 决策边界是假设函数的一个属性，它是由函数内的参数 **θ** 来决定。

* 更复杂的决策边界

> 假设有一训练集，其假设函数为：
> $$
> h_θ(x) = g(θ_0 + θ_1x_1+θ_2x_2 + θ_3x^2_1+θ_4x_2^2) \\[2ex]
> 设θ = \begin{bmatrix}-1 \\ 0 \\ 0 \\1\\1\end{bmatrix}
> $$
> 此函数中添加了两个属性 x<sub>1</sub><sup>2</sup>，x<sub>2</sub><sup>2</sup> ；
>
> 将 θ 带入假设函数有： 若 -1 + x<sub>1</sub><sup>2</sup>+ x<sub>2</sub><sup>2</sup> ≥ 0 ，则 y=1 ；或 x<sub>1</sub><sup>2</sup>+ x<sub>2</sub><sup>2</sup> ≥ 1 同理；
>
> ![image-20220502221634504](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220502221634504.png)
>
> 则 x<sub>1</sub><sup>2</sup>+ x<sub>2</sub><sup>2</sup> = 1  为该训练集的决策边界，圆内为 y=0, 圆外为 y=1，
>
> **如此而来，在假设函数中增加特征，使其变成复杂的多项式，也会得到更为复杂的决策边界。**

## 代价函数

* 如何拟合逻辑回归模型的参数 θ

> $$
> 之前的线性回归代价函数：J(θ)=\frac{1}{m}\sum_{i=1}^m\frac{1}{2}(h_θ(x^i) - y^i)^2\\[2ex]
> 令 ：cost(h_θ(x^i),y^i) = \frac{1}{2}(h_θ(x^i) - y^i)^2\\[2ex]
> $$
>
> cost函数中，对于实际值是 y ，但是学习算法给出的预测值是 h ，那我就需要让这个算法付出代价
>
> 若将线性回归的代价函数用于逻辑回归中，会使 J 函数为非凸函数，即存在多个局部最优解：
>
> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220502223128555.png" alt="image-20220502223128555" style="zoom:67%;" />
>
> 我们希望 J 函数为一个凸函数，即右图中的样子，当使用梯度下降时，会收敛到该函数的全局最小值。

* 逻辑回归代价函数
  $$
  cost(h_θ(x),y)  =\begin{cases}
  -log(h_θ(x)) & if\,\,\, y=1 \\
  -log(1-h_θ(x)) & if\,\,\, y=0 \\
  
  \end{cases}
  $$

  > 根据上面的公式计算代价，当预测和实际一致时代价为0，反之代价为无穷大。
  >
  > 当 y=1，h(x) = 1，即实际值与预测值一致时 ，Cost = 0 ，即代价为0；
  >
  > 当y =1， h=0（P(y=1|x;θ)=0），实际值与预测值不一致，就需要巨大代价惩罚这个函数；
  >
  > 即当 h(x) 趋于 0 时 ，代价值激增，则 Cost 趋于正无穷；
  >
  > <img src="https://img2018.cnblogs.com/blog/542362/201810/542362-20181030181937921-1956212769.png" alt="img" style="zoom: 80%;" />
  >
  > 当y=0 ，h=0，一致，Cost=0；
  >
  > 当y=0，h=1，不一致，同理；
  >
  > <img src="https://img2018.cnblogs.com/blog/542362/201810/542362-20181030182120690-232373933.png" alt="img" style="zoom: 80%;" />
  >
  > 总结：
  >
  > * **Cost(h<sub>θ</sub>(x),y) = 0 　if h<sub>θ</sub>(x)=y**
  >
  > * **Cost(h<sub>θ</sub>(x),y)→∞ 　if y=0 and h<sub>θ</sub>(x)→1**
  >
  > * **Cost(h<sub>θ</sub>(x),y)→∞ 　if y=1 and h<sub>θ</sub>(x)→0**

## 简化代价函数与梯度下降

$$
J(θ)=\frac{1}{m}\sum_{i=1}^mcost(h_θ(x^i),y^i)\\[2ex]
cost(h_θ(x),y)  =\begin{cases}
-log(h_θ(x)) & if\,\,\, y=1 \\
-log(1-h_θ(x)) & if\,\,\, y=0 \\

\end{cases}
$$

* 简化

  > $$
  > Cost(h_θ(x),y) = -ylog(h_θ(x)) - (1-y)log(1-h_θ(x)) \\[2ex]
  > 若y=1：Cost(h_θ(x),y) = -log(h_θ(x))\\[2ex]
  > 若y=0：Cost(h_θ(x),y) = - log(1-h_θ(x))
  > $$
  >
  > 完整的代价函数如下：
  > $$
  > J(θ)=-\frac{1}{m}[\sum_{i=1}^my^ilogh_θ(x^i)+(1-y^i)log(1-h_θ(x^i))]\\[2ex]
  > $$

  > 向量化实现：
  > $$
  > h=g(Xθ)\\[2ex]
  > J(θ)=\frac{1}{m}⋅(−y^Tlog(h)−(1−y)^Tlog(1−h))
  > $$
  > 

* 最小化代价函数J

  > 最小化代价函数的方法时适用梯度下降法：
  > $$
  > Want \,\,\,min_θJ(θ): \\[2ex]
  > Repeat\lbrace  \\[2ex]
  > θ_j:= θ_j-α\frac{∂}{∂θ_j}J(θ) \\[2ex]
  > \rbrace（θ_j同步更新）
  > $$
  > 将**偏导数**结果带入得：
  > $$
  > Want \,\,\,min_θJ(θ): \\[2ex]
  > Repeat\lbrace  \\[2ex]
  > θ_j:= θ_j-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j \\[2ex]
  > \rbrace（θ_j同步更新）
  > $$
  > 其中$h_θ=\frac {1} {1+e^{-θ^Tx}} $ ，也就是说，这与线性回归的梯度下降函数不完全一致。
  >
  > 梯度下降的向量化实现：
  > $$
  > θ:=θ− 
  > m
  > α
  > ​
  >  X ^T
  >  (g(Xθ)− 
  > y
  > ​
  >  )
  > $$

## 高级优化

* 有一些更加高级的算法用来最小化**J**函数： **共轭梯度** （Conjugate Gradient）， **局部优化法** （Broyden fletcher goldfarb shann,BFGS）和 **有限内存局部优化法** （L-BFGS）

> 这些算法有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 ；
>
> 只需要给这些算法提供计算导数项和代价函数的方法，就可以返回结果。适用于大型机器学习问题。
>
> 优点：
>
> * 不需要手动选择学习率α
> * 收敛速度快于梯度下降
>
> 缺点：
>
> * 更加复杂
>
> 它们太复杂，不应该自己实现，而是调用MATLAB方法。例如一个无约束最小值函数 fminunc 。它会使用众多高级优化算法中的一个，类似于加强版的梯度下降法，会自动选择学习速率，找到最佳的 Θ 值。

* 如果使用这些算法

> ***Example*:**
> $$
> θ = \begin{bmatrix}θ_1 \\ θ_2\end{bmatrix} \\[2ex]
> J(θ) = (θ_1-5)^2 + (θ_2 -5)^2 \\[2ex]
> \frac{∂}{∂θ_1}J(θ) = 2(θ_1-5)\\[2ex]
> \frac{∂}{∂θ_2}J(θ) = 2(θ_2-5)\\[2ex]
> $$
> 上述例子对应的函数：
>
> ```octave
> function [jVal, gradient] = costFunction(theta) % 该函数返回两个值
>   jVal = ((theta(1)-5)^2+(theta(2)-5)^2; % jval 代表J函数
>   gradient = zeros(2,1); % gradient 代表一个2*1的向量
>   gradient(1) = 2*(theta(1)-5); % 偏导数 
>   gradient(2) = 2*(theta(2)-5);
> ```
>
> 调用 高级优化函数 **fminunc**：
>
> ```octave
> options = optimset('GradObj', 'on', 'MaxIter', 100); % GradObj,on:设置梯度目标参数为打开
> 													% MaxIter,100: 最大迭代次数为100
> initialTheta = zeros(2,1); % 初识的θ参数
>    [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); % 无约束最小化函数
>    % @costFunction:@指向刚才定义的代价函数的指针
>    % initialTheta中的θ必需是d维向量，且d≥2
> ```
>
> 此函数会自动选择**学习率**，为你找到最佳的θ值。

> 运行代码：
>
> ```octave
> >> options = optimset('GradObj','on','MaxIter',100);
> >> initialTheta = zeros(2,1)
> initialTheta =
>    0
>    0
> >> [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, op
> tions)
> optTheta =
>    5
>    5
> functionVal = 7.8886e-31
> exitFlag = 1 
> ```
>
> **θ的最优解：theta(1)=5，theta(2)=5；**
>
> **`functionVal = 7.8886e-31` 表示10的-31次幂，基本上趋于0；**
>
> **`exitFlag = 1` 表示已经收敛。**

* 应用到逻辑回归中

> 假设给定参数：$theta= \begin{bmatrix}θ_0 \\ θ_1 \\\vdots\\θ_n \end{bmatrix}$
>
> 在**Octave**中，θ0对应 theta(1)，θ1对应 theta(2) ，...，θn对应 theta(n+1)
>
> 然后写出代价函数：
>
> ```octave
> function [jVal, gradient] = costFunction(theta) 
> jVal = [code to computeJ(θ)]; % J函数
> gradient(1) = [code to compute J关于θ0的偏导数] 
> gradient(2) = [code to compute J关于θ1的偏导数]
> ...
> gradient(n+1) = [code to compute J关于θn+1的偏导数]
> ```

## 多元分类：一对多

* 假如现在需要一个学习算法来解决以下问题

> 例1，对这四类邮件进行分类：工作、好友、家庭、兴趣爱好；
>
> 例2，对病人进行医学诊断：健康、感冒、流感；
>
> 例3，天气分类问题：晴天、多云、下雨、下雪；
>
> 以上问题都属于多分类问题。

* 二元分类与多元分类

  > <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220506112616863.png" alt="image-20220506112616863" style="zoom: 67%;" />
  >
  > 在二元分类中，用直线来将数据集分为正类和负类；
  >
  > 而在多元中，也能用此方法来进行分类。
  >
  > <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220506112829604.png" alt="image-20220506112829604" style="zoom:67%;" />
  >
  > 如图，有三个类别，y=1表示三角形，y=2表示正方形，y=3表示交叉；
  >
  > **我们要做的，就是将此数据集分为三个独立的二元分类问题，如右边图；**
  >
  > 如三角形中，创建一个新的数据集，同样用直线来区分正类和负类，圆形表示负样本，然后拟合一个分类器 $h_θ^1(x)$，则有三角形=1，圆形=0。
  >
  > 拟合出了三个分类器，分别对应i=1,2,3，估计给定x、θ后，y=i的概率；

> 多分类问题中，y 有 {0,1...n} 一共 n+1 中可能值。方法：
>
> * 拆分成 n+1 个二分类问题。
>
> * 对每个分类，都预测出一个h(x)值。代表 y是这个类型的可能性。
>
> * 最后结果为可能性最大的那个类型。
>
> $$
> y∈{0,1...n}\\[2ex]
> h^{(0)}_θ(x)=P(y=0|x;θ)\\[2ex]
> h^{(1)}_θ(x)=P(y=1|x;θ)\\[2ex]
> ⋯\\[2ex]
> h^{(n)}_θ(x)=P(y=n|x;θ)\\[2ex]
> prediction=max\,\,_i\,\,(h^{(i)}_θ(x)) (取h预测结果最大的那个i)
> $$

# 正则化

## 过拟合

* 关于线性回归的过拟合

> 
>
> 如图：<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220509155857583.png" alt="image-20220509155857583" style="zoom: 50%;" />随着房子面积增大，住房价格逐渐稳定或着说越往右越平缓，该算法没有很好地拟合数据集；
>
> 我们把这个问题成为**欠拟合**（*Underfit*），或这个算法具有**高偏差**，导致预测不准确。
>
> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220509160253151.png" alt="image-20220509160253151" style="zoom: 50%;" />
>
> 加入了二次函数后，拟合的效果就刚好合适（*Just Right*）；
>
> 当我们拟合一个四阶多项式，拟合的曲线可能会这样通过样本，如右图，这就是**过度拟合**（*Overfit*），或这个算法具有**高方差**，导致泛化能力差。
>
> **过拟合：变量过多时，假设函数能很好地拟合训练集，则代价函数J的结果可能非常接进0，但得到的曲线不够平滑，导致无法泛化到新的样本中。**（泛化：一个假设模型应用到新样本的能力）

* 关于逻辑回归的过拟合

> <img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220509162007791.png" alt="image-20220509162007791" style="zoom: 50%;" />
>
> 同理，图中分别是欠拟合，刚好合适，过拟合的曲线。

* 如何解决过拟合问题

> **Options**：
>
> * 减少特征
>   * 人工来选择哪些变量更为重要，哪些应该保留或舍弃；
>     * 缺点：舍弃了变量，就相当于舍弃了关于问题的一些信息。
>   * 模型选择算法（后面会讲到）
> * 正则化
>   * 保留所有特征，但是减少量级或参数$θ_j$的大小。

## 代价函数

* 正则化思想

> 设假设函数为：
> $$
> θ_0+θ_1x+θ_2x^2+θ_3x^3+θ_4x^4
> $$
> 当出现过拟合后，不妨在代价函数中加入惩罚项，使θ3和θ4都非常小：
> $$
> min_θ\,\,\frac{1}{2m}\sum_{i=1}^m(h_θ(x^i)-y^i)^2+1000θ_3^2+1000θ_4^2\\[2ex]
> $$
> 其中，1000只是一个比较大的数，它会使整个函数变得更大；
>
> 这时，我们需要使**θ3≈0，θ4≈0**，使其尽量接近于0，则假设函数会接近于：$θ_0+θ_1x+θ_2x^2$ ；
>
> 其本质上仍得到一个二次函数进行拟合。

> * **当我们的参数值$θ_0,θ_1,...,θ_n$较小时：**
>
>   * **意味着需要一个更简单的假设模型（加入惩罚项消除高阶项）**；
>
>   * **出现过拟合的概率变小。**

> 假设预测房屋价格中：
>
> 特征：$x_1,x_2,...,x_{100}$，含有大量特征；
>
> 参数：$θ_0,θ_1,...,θ_{100}$；
>
> 其中，并不知道哪些特征的相关性对预测结果来说较低，即参数中也不知道该选出哪些来缩小它们的值；
> $$
> J(θ)=\,\,\frac{1}{2m}[\sum_{i=1}^m(h_θ(x^i)-y^i)^2 + λ\sum_{i=1}^mθ_j^2]\\[2ex]
> $$
> 添加一个**正则化项**，此项的作用是缩小每一个参数；
>
> **λ** 称为正则化参数，用来控制两个不同目标之间的取舍。

* 正则化参数

> 如果λ设置得较大时，则对所有的θ来说，结果都会接近于0；
>
> 这样就相当于把假设函数的全部项都忽略掉了，最后会变成：$h_θ(x)=θ_0$ ；
>
> 结果就会是一条直线去拟合训练集<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220509222032534.png" alt="image-20220509222032534" style="zoom: 67%;" />，这就是出现了欠拟合。
>
> **所以在正则化中，要选择一个合适的参数值λ。**

## 线性回归的正则化

* 加入正则化的梯度下降公式

$$
Repeat\lbrace\\[2ex]
θ_0:=θ_0-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x_0^i\\[2ex]
θ_j:=θ_j-α[\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j+\frac{λ}{m}θ_j]\\[2ex]
\rbrace\\[2ex]
第二个式子可以转化为：θ_j:=θ_j(1-α\frac{λ}{m})-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j
$$

> 其中，$1-α\frac{λ}{m}<1$，是只比1略小一点的数，如0.99；
>
> 则$θ_j*0.99$ 就会把$θ_j$ 变得更小一点点，即θ平方范数变小了，而 $α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j$ 与之前的梯度下降公式是一样的。
>
> **当我们进行正则化线性回归时，每次迭代 $θ_j$ 都会乘以一个比1略小的数，每次都会缩小一点，然后进行后面的更新操作。**

* 加入正则化的正规方程

$$
X=\begin{bmatrix} (x^i)^T  \\ \vdots \\ (x^m)^T   \end{bmatrix} \,\,\,\,\,\,\, y=\begin{bmatrix} y^1  \\ \vdots \\ y^m  \end{bmatrix}
$$

> 使用正则化之前，先建立一个设计矩阵X（m*(n+1)），它的每一行都代表一个单独的训练样本；
>
> 然后建立一个向量y∈R<sup>m</sup>，它包含了训练集里的所有标签。
>
> 为了取$J(θ)$函数的最小值，令$θ=(X^TX+λ*L)^{-1}X^Ty$ ，其中，$L=\begin{bmatrix} 0\\ &1    \\&&1  \\ &&&\ddots\\&&&&1 \end{bmatrix}$ (n+1)*((n+1)维矩阵

## 逻辑回归的正则化

* 加入正则化项后的代价函数

$$
J(θ)=-\frac{1}{m}\sum_{i=1}^m[y^ilogh_θ(x^i)+(1-y^i)log(1-h_θ(x^i))]+   \frac{λ}{2m}\sum_{j=1}^nθ_j^2
\\[2ex]
$$

* 加入正则化项后的梯度下降函数

$$
Repeat\lbrace\\[2ex]
θ_0:=θ_0-α\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x_0^i\\[2ex]
θ_j:=θ_j-α[\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j+\frac{λ}{m}θ_j]\\[2ex](j=1,2,...n)\\[2ex]
\rbrace\\[2ex]
$$

> 其中，逻辑回归中的假设函数是：$h_θ(x)=\frac{1}{1+e^{-θ^Tx}}$ ；
>
> 而 $[\frac{1}{m}\sum_{i=1}^m(h_θ(x^i) - y^i)x^i_j+\frac{λ}{m}θ_j]$ 可以看成：$\frac{∂}{∂θ_j}J(θ)$ 。
>
> 

* 高级优化中使用正则化

<img src="https://img2018.cnblogs.com/blog/542362/201810/542362-20181031094521891-1602410675.png" alt="img" style="zoom: 67%;" />

> 定义costFunction函数，以theta作为输入，注意：在Octave中，theta的下标从1开始；
>
> 然后将代价函数赋给**fminunc**函数中；
>
> 在返回值 **jVal** 中，要在J函数中加入正则化项；
>
> 另一个返回值是**gradient** ，如令 gradient(1) 为J对$θ_0$的偏导数，以此类推；从gradient(2)开始，加入了正则化项。
>
> 在运行 fminunc 函数后，就会给出J函数的解。

* Python代码

```python
import numpy as np
def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:the
ta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg
```



# 神经网络的描述

